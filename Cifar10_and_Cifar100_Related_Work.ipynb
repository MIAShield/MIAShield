{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MIAShield/MIAShield/blob/main/Cifar10_and_Cifar100_Related_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hOEjeycVtF1"
      },
      "outputs": [],
      "source": [
        "#install all required library\n",
        "!sudo pip install Pillow\n",
        "!sudo pip install imagehash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAMA93SFEYyk"
      },
      "outputs": [],
      "source": [
        "# install the most recent verson of tensorflow-privacy\n",
        "#!pip install -U git+https://github.com/tensorflow/privacy\n",
        "!pip install tensorflow-privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4cM1wEEEd6t"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import glob\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import imagehash\n",
        "import hashlib\n",
        "\n",
        "\n",
        "#importing libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# general imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy import special\n",
        "\n",
        "# tensorflow imports\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCVoDpToEhFx",
        "outputId": "bc9b31a5-6bdc-404d-9059-5d63efada26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_privacy/privacy/membership_inference_attack/__init__.py:19: UserWarning: \n",
            "Membership inference attack sources were moved. Please replace\n",
            "import tensorflow_privacy.privacy.membership_inference_attack\n",
            "\n",
            "with\n",
            "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack\n",
            "  \"\\nMembership inference attack sources were moved. Please replace\"\n"
          ]
        }
      ],
      "source": [
        "# tensorflow-privacy libraries\n",
        "from tensorflow_privacy.privacy.membership_inference_attack import membership_inference_attack as mia\n",
        "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import AttackInputData\n",
        "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import SlicingSpec\n",
        "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import AttackType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset='Cifar100'    #dataset 'Cifar10' or 'Cifar100'"
      ],
      "metadata": {
        "id": "km6uIpYMloFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset=='Cifar10':\n",
        "  #define all the variables\n",
        "#cifar10\n",
        "  shape = (32, 32, 3)\n",
        "  num_class=10\n",
        "  learning_rate=.01\n",
        "  epochs=60\n",
        "elif dataset=='Cifar100':\n",
        "  #cifar100\n",
        "  shape = (32, 32, 3)\n",
        "  num_class=100\n",
        "  learning_rate=.01\n",
        "  epochs=130\n"
      ],
      "metadata": {
        "id": "XOIThxshlsUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128"
      ],
      "metadata": {
        "id": "rb_TI1M5JnyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBhp7Xf1JIyZ"
      },
      "outputs": [],
      "source": [
        "def load_cifar10():\n",
        "  \"\"\"Loads cifar10-Dataset and preprocesses to combine training and test data.\"\"\"\n",
        "  \n",
        "  # load the existing CIFAR10 dataset that comes in form of traing + test data and labels\n",
        "  train, test = tf.keras.datasets.cifar10.load_data()\n",
        "  train_data, train_labels = train\n",
        "  test_data, test_labels = test\n",
        "\n",
        "  # scale the images from color values 0-255 to numbers from 0-1 to help the training process\n",
        "  train_data1 = np.array(train_data, dtype=np.float32) / 255\n",
        "  test_data1 = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "  # cifar10 labels come one-hot encoded, there\n",
        "  train_labels = train_labels.flatten()\n",
        "  test_labels = test_labels.flatten()\n",
        "\n",
        "  return train_data1, train_labels, test_data1, test_labels, train_data, train_labels, test_data, test_labels "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar100():\n",
        "  \"\"\"Loads cifar100-Dataset and preprocesses to combine training and test data.\"\"\"\n",
        "  \n",
        "  # load the existing CIFAR10 dataset that comes in form of traing + test data and labels\n",
        "  train, test = tf.keras.datasets.cifar100.load_data()\n",
        "  train_data, train_labels = train\n",
        "  test_data, test_labels = test\n",
        "\n",
        "  # scale the images from color values 0-255 to numbers from 0-1 to help the training process\n",
        "  train_data1 = np.array(train_data, dtype=np.float32) / 255\n",
        "  test_data1 = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "  # cifar10 labels come one-hot encoded, there\n",
        "  train_labels = train_labels.flatten()\n",
        "  test_labels = test_labels.flatten()\n",
        "\n",
        "  return train_data1, train_labels, test_data1, test_labels, train_data, train_labels, test_data, test_labels "
      ],
      "metadata": {
        "id": "DHlh3nv8lMGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu_o11pnJV2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5dbad4-30b0-4016-ce10-36bebf3739a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n",
            "169017344/169001437 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "if dataset=='Cifar10':\n",
        "       train_data, train_labels, test_data, test_labels,trainX1,trainY1, testX1,testY1 = load_cifar10()  \n",
        "elif dataset=='Cifar100':\n",
        "       train_data, train_labels, test_data, test_labels,trainX1,trainY1, testX1,testY1 = load_cifar100() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNvolC7ZJcJF"
      },
      "outputs": [],
      "source": [
        "train_y=to_categorical(train_labels)\n",
        "test_y=to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKjzG7i7Jzbu"
      },
      "outputs": [],
      "source": [
        "def AlexnetModel(input_shape,num_classes):\n",
        "    model = Sequential()\n",
        "# model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
        "# for original Alexnet\n",
        "    model.add(Conv2D(48, (3,3), strides=(2,2), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(96, (3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHVhM8ISJfg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae4052d-af20-4966-b141-df9f5529a04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 16, 16, 48)        1344      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 8, 8, 48)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8, 8, 48)         192       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 96)          41568     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 3, 3, 96)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 3, 3, 96)         384       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 192)         166080    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 3, 192)         331968    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 3, 3, 256)         442624    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1, 1, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               25700     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,273,796\n",
            "Trainable params: 1,272,996\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# make the neural network model with the function specified above.\n",
        "# one model is supposed to train for 10, one for 50 epochs\n",
        "model_np = AlexnetModel(shape,num_class)\n",
        "model_np.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c7aA-MXJ9Gg"
      },
      "outputs": [],
      "source": [
        "#Run the model\n",
        "# specify parameters\n",
        "#learning_rate=.005\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# compile the model\n",
        "model_np.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg77yhhUKMKh"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "history = model_np.fit(train_data, train_y,\n",
        "                       validation_data=(test_data, test_y),\n",
        "                       batch_size=128, \n",
        "                       epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "print('Predict on train...')\n",
        "prob_train = model_np.predict(train_data[0:5000])\n",
        "print('Predict on test...')\n",
        "prob_test = model_np.predict(test_data[5000:10000])"
      ],
      "metadata": {
        "id": "JPvBFSoKajn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064c57a9-ade5-4cb8-d3b9-d823cd55a5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict on train...\n",
            "Predict on test...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=train_data[0:5000]\n",
        "test=test_data[5000:10000]"
      ],
      "metadata": {
        "id": "YCA3Jkv0XYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "ymP3hLmHPJTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4b3-2tmKTeJ"
      },
      "source": [
        "DP-SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFR9NQ7GMCE9"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy import special\n",
        "\n",
        "# tensorflow imports\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer,DPAdamGaussianOptimizer\n",
        "import tensorflow_privacy\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFp6hAVdNW1-"
      },
      "outputs": [],
      "source": [
        "l2_norm_clip = 1.5\n",
        "noise_multiplier = .248\n",
        "num_microbatches = 16\n",
        "learning_rate = 0.01\n",
        "batch_size=128\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "#loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "modelsgd=AlexnetModel(shape,num_class)\n",
        "modelsgd.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60Y6mHTYNiXa"
      },
      "outputs": [],
      "source": [
        "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=train_data.shape[0],\n",
        "                                              batch_size=batch_size,\n",
        "                                              noise_multiplier=.248,\n",
        "                                              epochs=epochs,\n",
        "                                              delta=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkV0-sIBOLrw"
      },
      "outputs": [],
      "source": [
        "history=modelsgd.fit(train_data, train_y,\n",
        "          epochs=epochs,\n",
        "          validation_data=(test_data[0:1000], test_y[0:1000]),\n",
        "          batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t164URmshwGY"
      },
      "outputs": [],
      "source": [
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "print('Predict on train...')\n",
        "prob_trainsgd = modelsgd.predict(train_data[0:5000])\n",
        "print('Predict on test...')\n",
        "prob_testsgd = modelsgd.predict(test_data[5000:10000])\n",
        "print('Compute losses...')\n",
        "cce = tf.keras.backend.categorical_crossentropy\n",
        "constant = tf.keras.backend.constant\n",
        "yt=train_labels[0:5000]\n",
        "ys=test_labels[5000:10000]\n",
        "y_train_onehot = to_categorical(yt)\n",
        "y_test_onehot = to_categorical(ys)\n",
        "\n",
        "#loss_train = cce(constant(y_train_onehot), constant(prob_train), from_logits=False).numpy()\n",
        "#loss_test = cce(constant(y_test_onehot), constant(prob_test), from_logits=False).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "RV_rHzlCo3E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PATE"
      ],
      "metadata": {
        "id": "IMBsxN1vQE19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "#ros = RandomOverSampler()\n",
        "\n",
        "#X_ros, y_ros = ros.fit_resample(x_train_split0, trainy_split0)\n",
        "# split x_train to 5 disjoint datasets\n",
        "#store each dataset variable name is the list Xtrain\n",
        "M=25\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "X=train_data\n",
        "Y=train_labels\n",
        "for i in range(0,M):\n",
        "  K=M-i\n",
        "  if(K>=2):\n",
        "    train=X\n",
        "    test=Y\n",
        "    skf = StratifiedKFold(n_splits=K)\n",
        "    skf.get_n_splits(X, Y)\n",
        "    #print(skf)\n",
        "    for train_index, test_index in skf.split(X, Y):\n",
        "       #print(X.shape)\n",
        "       X,globals()['x_train_pate%s' % i]=train[train_index], train[test_index]\n",
        "       Y,globals()['patey_split%s' % i]=test[train_index], test[test_index]\n",
        "       y_hot=globals()['patey_split%s' % i]\n",
        "       y_hot=to_categorical(y_hot)\n",
        "       globals()['y_train_pate%s' % i]=y_hot\n",
        "       #X=train\n",
        "       #Y=test\n",
        "  elif K==1:\n",
        "    globals()['x_train_pate%s' % i]=X\n",
        "    globals()['patey_split%s' % i]=Y\n",
        "    y_hot=to_categorical(Y)\n",
        "    globals()['y_train_pate%s' % i]=y_hot"
      ],
      "metadata": {
        "id": "GIoyKnGSQF2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(unique, counts) = np.unique(patey_split0, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "frequencies"
      ],
      "metadata": {
        "id": "Sx6ERAIxljuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_index =0\n",
        "for iter in range(M):\n",
        "  modelpate=AlexnetModel(shape,num_class)\n",
        "  #modelpate.build(input_shape = (None,32,32,3))#AlexnetModel(shape,num_class)\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  modelpate.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  modelpate.fit(globals()['x_train_pate%s' % iter], globals()['y_train_pate%s' % iter], validation_data=(test_data, test_y),\n",
        "                       batch_size=128, \n",
        "                       epochs=epochs)\n",
        "  globals()['model_pate%s' % iter]=modelpate"
      ],
      "metadata": {
        "id": "v8kr4uv_VSnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate accuracy/utility test acc\n",
        "prb=model_pate29.predict(test_data)\n",
        "test_labeld_result=np.argmax(prb,axis=1)\n",
        "accuracy_score(test_labels,test_labeld_result)\n",
        "#prob_testd"
      ],
      "metadata": {
        "id": "gq8HneKIlD9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return final"
      ],
      "metadata": {
        "id": "5WDzP58R2w8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate parameters from our combined\n",
        "#from joblib import Parallel, delayed\n",
        "import time\n",
        "def calc_pate(data1,ep):\n",
        "  n=len(data1)\n",
        "  #logits=np.zeros((n,10))\n",
        "  prob=np.zeros((n,num_class))\n",
        "  labels=np.zeros(n)\n",
        "  #index_resulted=np.zeros(n)\n",
        "  #ts=.02\n",
        "  kk=0\n",
        "  sum2=np.zeros((n,num_class))\n",
        "    #index=model_index(d2,5)\n",
        "    #index_resulted[p1]=index\n",
        "    #print(index)\n",
        "  for p2 in range(M):\n",
        "        modelh=globals()['model_pate%s' % p2]\n",
        "        prob_d = modelh.predict(data1)\n",
        "        #prob_d = special.softmax(logits_d)\n",
        "        kk=kk+1\n",
        "        #sum1=logits_d+sum1\n",
        "        prob_vote=vote(prob_d)\n",
        "        sum2=prob_vote+sum2\n",
        "  sum_noise=sum2+np.random.laplace(loc=0.0, scale=1/ep)\n",
        "  prob=sum_noise/M\n",
        "  prob=vote(prob)\n",
        "    #labels[p1,:]=np.argmax(logits[p1,:])\n",
        "  return prob\n"
      ],
      "metadata": {
        "id": "JgXKfoZ7W8rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ep=[0.01,1,100]"
      ],
      "metadata": {
        "id": "h0DjElTjxKdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vote(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return c"
      ],
      "metadata": {
        "id": "7qKPyJaixRxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate over training and testing\n",
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "import timeit\n",
        "print('Predict on train...')\n",
        "start1 = timeit.default_timer()\n",
        "#logits_traind,prob_traind,train_index = calc_attack_matrix(train_data[0:5000],trainX1[0:5000])\n",
        "prob_trainp= calc_pate(train_data[0:5000],ep[1])\n",
        "stop1 = timeit.default_timer()\n",
        "print('time required for members',stop1-start1)\n",
        "print('Predict on test...')\n",
        "start2 = timeit.default_timer()\n",
        "prob_testp= calc_pate(test_data[5000:10000],ep[1])\n",
        "stop2 = timeit.default_timer()\n",
        "print('time required for non-members',stop2-start2)\n"
      ],
      "metadata": {
        "id": "Ef9qlRBiWQlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "1ZeYVyjV5ZxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memuard"
      ],
      "metadata": {
        "id": "5wOVAiRlndUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def memguard(data):\n",
        "  \"\"\" Given confidence vectors, perform memguard post processing to protect from membership inference.\n",
        "  Note that this defense assumes the strongest defender that can make arbitrary changes to the confidence vector\n",
        "  so long as it does not change the label. We as well have the (weaker) constrained optimization that will be\n",
        "  released at a future data.\n",
        "  Args:\n",
        "    scores: confidence vectors as 2d numpy array\n",
        "  Returns: 2d scores protected by memguard.\n",
        "  \"\"\"\n",
        "  print('Predict...')\n",
        "  scores = model_np.predict(data)\n",
        "  n_classes = scores.shape[1]\n",
        "  epsilon = 1e-3\n",
        "  on_score = (1. / n_classes) + epsilon\n",
        "  off_score = (1. / n_classes) - (epsilon / (n_classes - 1))\n",
        "  predicted_labels = np.argmax(scores, axis=-1)\n",
        "  defended_scores = np.ones_like(scores) * off_score\n",
        "  defended_scores[np.arange(len(defended_scores)), predicted_labels] = on_score\n",
        "  return defended_scores"
      ],
      "metadata": {
        "id": "sJnoK9dInerk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Get Secure Probability Score from MemGuard')\n",
        "\n",
        "#print('Predict on test...')\n",
        "#prob_test= model_np.predict()\n",
        "\n",
        "print(\"Calculate MemGuard\")\n",
        "prob_trainMG = memguard(train)\n",
        "prob_testMG = memguard(test)"
      ],
      "metadata": {
        "id": "HrQzmGtdoHFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-Stacking"
      ],
      "metadata": {
        "id": "124g6bslCbhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=2)\n",
        "skf.get_n_splits(train_data, train_labels)\n",
        "for train_index, test_index in skf.split(train_data, train_labels):\n",
        "       train_data1,train_data2=train_data[train_index], train_data[test_index]\n",
        "       train_labels1,train_labels2=train_labels[train_index], train_labels[test_index]"
      ],
      "metadata": {
        "id": "kD6LjYk9CdSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model1\n",
        "model1=AlexnetModel(shape,num_class)\n",
        "#model1=make_model()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "#train data for model1\n",
        "train_y1=to_categorical(train_labels1)\n",
        "\n",
        "# compile the model\n",
        "model1.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "history1 = model1.fit(train_data1, train_y1,\n",
        "                       validation_data=(test_data, test_y),\n",
        "                       batch_size=128, \n",
        "                       epochs=epochs)\n"
      ],
      "metadata": {
        "id": "BV_OLyhjSVjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "#train data for model2\n",
        "model2 = RandomForestClassifier(n_estimators=40)\n",
        "dataF=train_data2.reshape(len(train_data2),32*32*3)\n",
        "dataS=test_data.reshape(len(test_data),32*32*3)\n",
        "model2.fit(dataF, train_labels2)\n",
        "y2=model2.predict(dataS)\n",
        "accuracy_score(y2,test_labels)"
      ],
      "metadata": {
        "id": "7UuQsGC5UBsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_models=[model1,model2]"
      ],
      "metadata": {
        "id": "mh9xXW-XU_Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stacked_dataset(d1):\n",
        "  y1=model1.predict(d1)\n",
        "  #y1=np.argmax(y1,axis=1)\n",
        "  d2=d1.reshape(len(d1),32*32*3)\n",
        "  y2=model2.predict_proba(d2)\n",
        "  #y=np.hstack((y1,y2))\n",
        "  y=np.concatenate((y1, y2),axis=1)\n",
        "  print(y.shape)\n",
        "  #stackX=np.vstack((d2,y))\n",
        "  return y"
      ],
      "metadata": {
        "id": "Co-eJoq3VCOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_stacked_model(members, inputX, inputy):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(inputX)\n",
        "\tmodels = LogisticRegression()\n",
        "\tmodels.fit(stackedX, inputy)\n",
        "\treturn models,stackedX"
      ],
      "metadata": {
        "id": "upW9XDo7VEG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit stacked model using the ensemble\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "modelS,stackedX = fit_stacked_model(base_models, test_data[5000:10000], test_labels[5000:10000])"
      ],
      "metadata": {
        "id": "KE1phx5gVGk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on test set\n",
        "yhat =modelS.predict(stackedX) #stacked_prediction(base_models, model, test_labels[5000:10000])\n",
        "acc = accuracy_score(test_labels[5000:10000], yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "VGbPA6lDVJ8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_stacking(data,model):\n",
        "  dataX=stacked_dataset(data)\n",
        "  prob=model.predict_proba(dataX)\n",
        "  return prob"
      ],
      "metadata": {
        "id": "QGZ6XGlDVwyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "print('Predict on train...')\n",
        "prob_trainS = model_stacking(train,modelS)\n",
        "print('Predict on test...')\n",
        "prob_testS = model_stacking(test,modelS)\n",
        "print('Compute losses...')\n",
        "cce = tf.keras.backend.categorical_crossentropy\n",
        "constant = tf.keras.backend.constant\n",
        "#yt=trainy_split0[0:nd]\n",
        "#ys=test_labels[0:nd]\n",
        "#y_train_onehot = to_categorical(yt)\n",
        "#y_test_onehot = to_categorical(ys)"
      ],
      "metadata": {
        "id": "miz_Bm5xV1A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "ZV3oMW01QXSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MixUp without MMD"
      ],
      "metadata": {
        "id": "eOZE3DakG9AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D, \\\n",
        "Dense, Input, Activation, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "import sklearn.metrics\n",
        "from numpy.random import default_rng\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "rng = default_rng()"
      ],
      "metadata": {
        "id": "0-FahI9ZG-9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the data into tensorflow dataset object\n",
        "# Put aside a few samples to create our validation set\n",
        "val_samples = 2000\n",
        "x_val, y_val = test_data[:val_samples], test_y[:val_samples]\n",
        "#new_x_train, new_y_train = train_data[val_samples:], train_y[val_samples:]\n",
        "\n",
        "train_ds_one = (\n",
        "    tf.data.Dataset.from_tensor_slices((train_data, train_y))\n",
        "    .shuffle(batch_size * 100)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "train_ds_two = (\n",
        "    tf.data.Dataset.from_tensor_slices(((train_data, train_y)))\n",
        "    .shuffle(batch_size * 100)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "\n",
        "train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_y)).batch(batch_size)"
      ],
      "metadata": {
        "id": "WcxAUvEzJExn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.3, concentration_1=0.3):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)"
      ],
      "metadata": {
        "id": "ocaNNT6HJxtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mix_up(ds_one, ds_two, alpha=0.3):\n",
        "    # Unpack two datasets\n",
        "    images_one, labels_one = ds_one\n",
        "    images_two, labels_two = ds_two\n",
        "    batch_size = tf.shape(images_one)[0]\n",
        "\n",
        "    # Sample lambda and reshape it to do the mixup\n",
        "    l = sample_beta_distribution(batch_size, alpha, alpha)\n",
        "    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n",
        "    y_l = tf.reshape(l, (batch_size, 1))\n",
        "\n",
        "    # Perform mixup on both images and labels by combining a pair of images/labels\n",
        "    # (one from each dataset) into one image/label\n",
        "    images = images_one * x_l + images_two * (1 - x_l)\n",
        "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
        "    return (images, labels)"
      ],
      "metadata": {
        "id": "AC4TFF-dJ0-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_mu = train_ds.map(\n",
        "    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.3), num_parallel_calls=AUTO\n",
        ")"
      ],
      "metadata": {
        "id": "E8bh5lqGJ45j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mix=AlexnetModel(shape,num_class)\n",
        "model_mix.summary()"
      ],
      "metadata": {
        "id": "9snDvIXrJ9Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mix.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
        "model_mix.fit(train_ds_mu, validation_data=val_ds, epochs=epochs)"
      ],
      "metadata": {
        "id": "HnyHirfHKKU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "print('Predict on train...')\n",
        "prob_trainMi = model_mix.predict(train)\n",
        "print('Predict on test...')\n",
        "prob_testMi = model_mix.predict(test)\n",
        "print('Compute losses...')\n",
        "cce = tf.keras.backend.categorical_crossentropy\n",
        "constant = tf.keras.backend.constant"
      ],
      "metadata": {
        "id": "pnACtHkvLgyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0ace3c-9129-4758-8a55-c3694c773452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict on train...\n",
            "Predict on test...\n",
            "Compute losses...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MMD+MixUp"
      ],
      "metadata": {
        "id": "SJ8X8ZjG2Lg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(train_data)"
      ],
      "metadata": {
        "id": "HuvCU7IW45YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D, \\\n",
        "Dense, Input, Activation, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import sklearn.metrics\n",
        "\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng()\n",
        "AUTO = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "0uWUCxjI4Z0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_samples = 2000\n",
        "x_val, y_val = train_data[30000:40000], train_y[30000:40000]"
      ],
      "metadata": {
        "id": "-Nzz3cpaGcsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)"
      ],
      "metadata": {
        "id": "0e4sEuckLkxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in val_ds:  # only take first element of dataset\n",
        "    x_val = images.numpy()\n",
        "    y_val = labels.numpy()"
      ],
      "metadata": {
        "id": "5kkzvrulLmQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.3, concentration_1=0.3):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)"
      ],
      "metadata": {
        "id": "-hTByH4R4i2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mix_up(ds_one, ds_two, alpha=0.3):\n",
        "    # Unpack two datasets\n",
        "    images_one, labels_one = ds_one\n",
        "    images_two, labels_two = ds_two\n",
        "    batch_size = tf.shape(images_one)[0]\n",
        "\n",
        "    # Sample lambda and reshape it to do the mixup\n",
        "    l = sample_beta_distribution(batch_size, alpha, alpha)\n",
        "    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n",
        "    y_l = tf.reshape(l, (batch_size, 1))\n",
        "\n",
        "    # Perform mixup on both images and labels by combining a pair of images/labels\n",
        "    # (one from each dataset) into one image/label\n",
        "    images = images_one * x_l + images_two * (1 - x_l)\n",
        "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "zl1RI2-s4mu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, images, labels, batch_size=64, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.key_array = np.arange(self.images.shape[0], dtype=np.uint32)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.key_array)//self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        keys = self.key_array[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        x = np.asarray(self.images[keys], dtype=np.float32)\n",
        "        y = np.asarray(self.labels[keys], dtype=np.float32)\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.key_array = np.random.permutation(self.key_array)"
      ],
      "metadata": {
        "id": "WRYIBX5M4qKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator1 = DataGenerator(images=train_data[0:int(n/2)], labels=train_y[0:int(n/2)], batch_size=batch_size, shuffle=True)\n",
        "generator2 = DataGenerator(images=train_data[int(n/2):n], labels=train_y[int(n/2):n], batch_size=batch_size, shuffle=True)\n",
        "n_batches = len(generator1)"
      ],
      "metadata": {
        "id": "YivvEg_14yAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_batches)"
      ],
      "metadata": {
        "id": "52cLl_NGq-KN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a684701f-b415-4556-ec1a-ac6f1d26e9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-remediation"
      ],
      "metadata": {
        "id": "s44AvonjC506"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs=epochs\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "loss_train = np.zeros(shape=(n_epochs,), dtype=np.float32)\n",
        "acc_train = np.zeros(shape=(n_epochs,), dtype=np.float32)\n",
        "loss_val = np.zeros(shape=(n_epochs,))\n",
        "acc_val = np.zeros(shape=(n_epochs,))"
      ],
      "metadata": {
        "id": "w55xDVLlCdKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_remediation import min_diff\n",
        "import tensorflow as tf\n",
        "mmd_loss=min_diff.losses.MMDLoss()"
      ],
      "metadata": {
        "id": "qqyVkALqDGc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ds(new_x_train,new_y_train):\n",
        "  train_ds_one = (\n",
        "    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
        "    .shuffle(batch_size * 100)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "  train_ds_two = (\n",
        "    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
        "    .shuffle(batch_size * 100)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "  train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
        "  return train_ds"
      ],
      "metadata": {
        "id": "CmIpYLM4DJ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_img(train_ds_mu):\n",
        "  for images, labels in train_ds_mu:  # only take first element of dataset\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    #l = l.numpy()\n",
        "    return numpy_images,numpy_labels"
      ],
      "metadata": {
        "id": "AdNOW-FgDN8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model \n",
        "model_mmd=AlexnetModel(shape,num_class)\n",
        "val_train_dif=30"
      ],
      "metadata": {
        "id": "cTilOU5cDSRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean() # Keeping track of the training loss\n",
        "  epoch_acc_avg = tf.keras.metrics.Mean() # Keeping track of the training accuracy\n",
        "\n",
        "  print('==== Epoch #{0:3d} ===='.format(epoch),'/',n_epochs)\n",
        "\n",
        "  for batch in tqdm(range(n_batches)):\n",
        "    x, y = generator1[batch]\n",
        "    x1, y1 = generator2[batch]\n",
        "    ds=train_ds(x,y)\n",
        "    ds1=train_ds(x1,y1)\n",
        "    tds= ds.map(lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO)\n",
        "    tds1= ds1.map(lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO)\n",
        "    xm,ym=numpy_img(tds)\n",
        "    #xm1,ym1=numpy_img(tds1)\n",
        "    #print(x.shape)\n",
        "    #x,y=mix_upn(x, y, alpha=0.3)\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape: # Forward pass\n",
        "      y_ = model_mmd(xm, training=True)\n",
        "      y1_ = model_mmd(x1, training=True)\n",
        "      yx_ = model_mmd(x, training=True)\n",
        "      #print(type(y_))\n",
        "      #print(y)\n",
        "      loss1 = ce_loss(y_, ym)\n",
        "      loss2 = mmd_loss(yx_, y1_)\n",
        "      #print(loss1)\n",
        "      if val_train_dif>3:\n",
        "        loss=loss2+loss1\n",
        "      elif val_train_dif<=3:\n",
        "        loss=loss1\n",
        "      #loss = compute_mmd(y1_,y_)/1000\n",
        "      #losst += loss\n",
        "    grad = tape.gradient(loss, model_mmd.trainable_variables) # Backpropagation\n",
        "    optimizer.apply_gradients(zip(grad, model_mmd.trainable_variables)) # Update network weights\n",
        "\n",
        "\n",
        "    epoch_loss_avg(loss)\n",
        "    epoch_acc_avg(sklearn.metrics.accuracy_score(y_true=np.argmax(ym, axis=-1), y_pred=np.argmax(y_, axis=-1)))\n",
        "    \n",
        "  generator1.on_epoch_end()\n",
        "\n",
        "  loss_train[epoch] = epoch_loss_avg.result()\n",
        "  acc_train[epoch] = epoch_acc_avg.result()\n",
        "\n",
        "  print('---- Training ----')\n",
        "  print('Loss  =  {0:.3f}'.format(loss_train[epoch]))\n",
        "  print('Acc   =  {0:.3f}'.format(acc_train[epoch]))\n",
        "\n",
        "  y_v = model_mmd.predict(x_val) # Validation predictions\n",
        "  loss_val[epoch] = ce_loss(y_val, y_v).numpy()\n",
        "  y_true=np.argmax(y_val, axis=-1)\n",
        "  #print(y_true)\n",
        "  acc_val[epoch] = sklearn.metrics.accuracy_score(y_true=np.argmax(y_val, axis=-1), y_pred=np.argmax(y_v, axis=-1))\n",
        "  val_train_dif=(abs(acc_val[epoch]-acc_train[epoch]))*100\n",
        "  print(val_train_dif)\n",
        "\n",
        "  print('--- Validation ---')\n",
        "  print('Loss  =  {0:.3f}'.format(loss_val[epoch]))\n",
        "  print('Acc   =  {0:.3f}'.format(acc_val[epoch]))"
      ],
      "metadata": {
        "id": "LCGI9NqZCumn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "print('Predict on train...')\n",
        "prob_trainMi = model_mmd.predict(train)\n",
        "print('Predict on test...')\n",
        "prob_testMi = model_mmd.predict(test)\n",
        "print('Compute losses...')\n",
        "cce = tf.keras.backend.categorical_crossentropy\n",
        "constant = tf.keras.backend.constant"
      ],
      "metadata": {
        "id": "oyPwJxj9duh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfb4544-3e29-446b-9768-14e4625d3f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict on train...\n",
            "Predict on test...\n",
            "Compute losses...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nmKPIodjmYa"
      },
      "source": [
        "Prob-Attack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_trainf=prob_trainMi\n",
        "prob_testf=prob_testMi\n",
        "yt=train_labels[0:5000]\n",
        "ys=test_labels[5000:10000]\n",
        "attack=3"
      ],
      "metadata": {
        "id": "I2xevbPqo09f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(unique, counts) = np.unique(ys, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "#frequencies"
      ],
      "metadata": {
        "id": "if0y37mb0_lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHVnqAK7iiRa"
      },
      "outputs": [],
      "source": [
        "# define what variables our attacker should have access to\n",
        "attack_input = AttackInputData(\n",
        "  logits_train = prob_trainf,\n",
        "  logits_test = prob_testf,\n",
        "  #loss_train = loss_train,\n",
        "  #loss_test = loss_test,\n",
        "  labels_train =yt,\n",
        "  labels_test =ys\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djN7_GlKjqRk"
      },
      "outputs": [],
      "source": [
        "# how should the data be sliced\n",
        "slicing_spec = SlicingSpec(\n",
        "    entire_dataset = True,\n",
        "    by_class = False,\n",
        "    by_percentiles = False,\n",
        "    by_classification_correctness = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHPOZS5anqre"
      },
      "outputs": [],
      "source": [
        "#Give which type of attack you are running\n",
        "#attack=3 #1=th, 2=LR, 3=MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Fu9W4MoDYn"
      },
      "outputs": [],
      "source": [
        "# define the type of attacker model that we want to use\n",
        "if attack==1:\n",
        "  attack_types = [\n",
        "    AttackType.THRESHOLD_ATTACK\n",
        "]\n",
        "elif attack==2:\n",
        "  attack_types = [\n",
        "    AttackType.LOGISTIC_REGRESSION\n",
        "]\n",
        "elif attack==3:\n",
        "  attack_types = [\n",
        "    AttackType.MULTI_LAYERED_PERCEPTRON\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byqTER1JoGbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f22bfb-d0d9-4a81-a70b-5b113adb4d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "# run the attack\n",
        "attacks_result = mia.run_attacks(attack_input=attack_input,\n",
        "                                 slicing_spec=slicing_spec,\n",
        "                                 attack_types=attack_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8swwgsvoNW3"
      },
      "outputs": [],
      "source": [
        "# summary by data slice (the best performing attacks per slice are presented)\n",
        "print(attacks_result.summary(by_slices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0IdnoUfoQ0h"
      },
      "outputs": [],
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_labeld_result=np.argmax(prob_testf,axis=1)\n",
        "accuracy_score(ys,test_labeld_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPpBylVHoe_Y"
      },
      "outputs": [],
      "source": [
        "#calculate train accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "train_labeld_result=np.argmax(prob_trainf,axis=1)\n",
        "accuracy_score(yt,train_labeld_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "tkqc50fq1wWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8V4WsPXon1I"
      },
      "source": [
        "GAP Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIvfIWMtojQS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdquAWxkoq-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c4ba0c-a998-4fa2-adde-2fb539ab814e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 802
        }
      ],
      "source": [
        "in1=np.zeros(5000)\n",
        "in2=np.ones(5000)\n",
        "Attack_target=np.hstack((in2,in1))\n",
        "Attack_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNbK9ILBovJP"
      },
      "outputs": [],
      "source": [
        "def Gap_attack(prob_train,prob_test,label_train,label_test):\n",
        "  result1=np.argmax(prob_train,axis=1)\n",
        "  result2=np.argmax(prob_test,axis=1)\n",
        "  result_label=np.hstack((result1,result2))\n",
        "  target_label=np.hstack((label_train,label_test))\n",
        "  target_label=np.array(target_label)\n",
        "  g1=len(target_label)\n",
        "  target_label=target_label.reshape(g1,1)\n",
        "  result_label=np.array(result_label)\n",
        "  result_label=result_label.reshape(g1,1)\n",
        "  target_label=target_label.astype(int)\n",
        "  result_label=result_label.astype(int)\n",
        "  attack_result=np.equal(target_label, result_label)\n",
        "  attack_result=attack_result.astype(int) \n",
        "  fpr, tpr, thresholds = metrics.roc_curve(Attack_target, attack_result, pos_label=1)\n",
        "  AUC=metrics.auc(fpr, tpr)\n",
        "  advantage=np.max(abs(fpr-tpr))\n",
        "  return AUC, advantage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0zk66XBoy6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa79d892-0cf7-4b4e-ce1a-0edb2a40b735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6191\n",
            "0.23819999999999997\n"
          ]
        }
      ],
      "source": [
        "#gap attack for non-private data\n",
        "auc,adv=Gap_attack(prob_trainf,prob_testf,yt,ys)\n",
        "print(auc)\n",
        "print(adv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "zwjDOQO-pME5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjS5V5IS2RmQ"
      },
      "source": [
        "Label-Only Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xV8f1FA2Tbh"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage, misc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnBProCjCQ4S"
      },
      "outputs": [],
      "source": [
        "#initialize the flags\n",
        "r=5\n",
        "target_model=model_np \n",
        "private_model=model_mmd #the model that needs to be evaluated\n",
        "train=train_data[5000:10000]\n",
        "test=test_data[0:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EfQ6z96CUcU"
      },
      "outputs": [],
      "source": [
        "def make_fc():\n",
        "  \"\"\"Return conv_model as tf.keras.Model, without Softmax layer (only logits).\n",
        "  To perform\n",
        "  :param input_shape:\n",
        "  :param depth:\n",
        "  :param regularization:\n",
        "  :param reg_constant:\n",
        "  :param num_classes:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  fc_model = tf.keras.models.Sequential()\n",
        "  #fc_model.add(tf.keras.layers.Dense(num_classes, kernel_regularizer=k_reg))\n",
        "  fc_model.add(tf.keras.layers.Dense(10, activation=tf.keras.layers.ReLU(\n",
        "        negative_slope=1e-2), kernel_initializer='glorot_normal'))\n",
        "  fc_model.add(keras.layers.Dense(10, activation=tf.keras.layers.ReLU(\n",
        "        negative_slope=1e-2), kernel_initializer='glorot_normal'))\n",
        "  fc_model.add(tf.keras.layers.Dense(2, kernel_initializer='glorot_normal'))\n",
        "  fc_model.add(tf.keras.layers.Softmax())\n",
        "  return fc_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdXXfibNCX9t"
      },
      "outputs": [],
      "source": [
        "def rotate_image(x1,a):  #angle\n",
        "  x2=np.zeros(x1.shape)\n",
        "  x3=np.zeros(x1.shape)\n",
        "  tp=x1.dtype\n",
        "  x2=x2.astype(tp)\n",
        "  x3=x3.astype(tp)\n",
        "  for i in range(len(x1)):\n",
        "    out1=ndimage.rotate(x1[i],a,reshape=False)\n",
        "    out2=ndimage.rotate(x1[i],-a,reshape=False)\n",
        "    x2[i]=out1\n",
        "    x3[i]=out2\n",
        "  return x2,x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OGPbDvcCYDg"
      },
      "outputs": [],
      "source": [
        "train_data_rotate1,train_data_rotate2=rotate_image(train_data[5000:10000],r)\n",
        "test_data_rotate1,test_data_rotate2=rotate_image(test_data[0:5000],r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjn9p8mNCiEt"
      },
      "outputs": [],
      "source": [
        "def calc_b(target_model,data,t):\n",
        "  n=len(data)\n",
        "  ytr=target_model.predict(data)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb8S7CGLCuuD"
      },
      "outputs": [],
      "source": [
        "t=train_labels[5000:10000].reshape(5000,1)\n",
        "s=test_labels[0:5000].reshape(5000,1)\n",
        "T=train_data[5000:10000].reshape(5000,32*32*3)\n",
        "S=test_data[0:5000].reshape(5000,32*32*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuibhqLKCz6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec5a2aa-f58d-4909-91cc-65a3e69310a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 813
        }
      ],
      "source": [
        "#calculate b\n",
        "btr1=calc_b(target_model,train_data_rotate1,t)\n",
        "btr2=calc_b(target_model,train_data_rotate2,t)\n",
        "btr3=calc_b(target_model,train,t)\n",
        "\n",
        "\n",
        "bsr1=calc_b(target_model,test_data_rotate1,s)\n",
        "bsr2=calc_b(target_model,test_data_rotate2,s)\n",
        "bsr3=calc_b(target_model,test,s)\n",
        "\n",
        "out1=np.ones(5000)\n",
        "out2=np.zeros(5000)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "#rearrange b\n",
        "btr=np.hstack((btr3,btr1,btr2))\n",
        "#btr=np.transpose(btr)\n",
        "#print(btr.shape)\n",
        "bsr=np.hstack((bsr3,bsr1,bsr2))\n",
        "#bsr=np.transpose(bsr)\n",
        "b=np.vstack((btr,bsr))\n",
        "b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fjUd6YzC3iB"
      },
      "outputs": [],
      "source": [
        "total=np.concatenate([t, s])\n",
        "Bt=np.hstack((b,total))\n",
        "Bt.shape\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvqA6wfuC8qD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "t_data,s_data,t_label,s_label=train_test_split(Bt, label, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-R8vSCgF6uS"
      },
      "outputs": [],
      "source": [
        "#train a model\n",
        "modelLB=make_fc()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# compile the model\n",
        "modelLB.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "# train the model\n",
        "history = modelLB.fit(t_data, t_label,\n",
        "                       validation_data=(s_data, s_label),\n",
        "                       batch_size=32, \n",
        "                       epochs=30)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUYi3FJPGChP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(s_data)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,s_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU5KxGgkGG-O"
      },
      "outputs": [],
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, s_label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC30WC_1GMCY"
      },
      "outputs": [],
      "source": [
        "np.max(abs(fpr-tpr))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "6rpaCDK8M3Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(target_model,data,t):    #if only MemGuard\n",
        "  n=len(data)\n",
        "  ytr=memguard(data)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "nfkPVcDo5A0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(private_model,data,t):    #if only model_stacking\n",
        "  n=len(data)\n",
        "  ytr=model_stacking(data,private_model)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "t8pxNM1Sdd8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(target_model,data,t):    #if only PATE\n",
        "  n=len(data)\n",
        "  ytr=calc_pate(data,ep[0])\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "FXSqa4I2C3is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create training and testing rotate data\n",
        "train_data_rotateT1,train_data_rotateT2=rotate_image(train_data[0:5000],r)\n",
        "#train_X1_rotateT1,train_X1_rotateT2=rotate_image(x_traind0[0:5000],r)\n",
        "test_data_rotateT1,test_data_rotateT2=rotate_image(test_data[5000:10000],r)\n",
        "#test_X1_rotateT1,test_X1_rotateT2=rotate_image(testX1[5000:10000],r)"
      ],
      "metadata": {
        "id": "WsCVBfRDLB1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=train_labels[0:5000].reshape(5000,1)\n",
        "s=test_labels[5000:10000].reshape(5000,1)\n",
        "T=train_data[0:5000].reshape(5000,32*32*3)\n",
        "S=test_data[5000:10000].reshape(5000,32*32*3)"
      ],
      "metadata": {
        "id": "9mQnPhwkLnmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate b\n",
        "Btr1=calc_b(private_model,train_data_rotateT1,t)\n",
        "Btr2=calc_b(private_model,train_data_rotateT2,t)\n",
        "Btr3=calc_b(private_model,train_data[0:5000],t)\n",
        "\n",
        "Bsr1=calc_b(private_model,test_data_rotateT1,s)\n",
        "Bsr2=calc_b(private_model,test_data_rotateT2,s)\n",
        "Bsr3=calc_b(private_model,test_data[5000:10000],s)\n",
        "\n",
        "out1=np.ones(5000)\n",
        "out2=np.zeros(5000)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "#rearrange b\n",
        "Btr=np.hstack((Btr3,Btr1,Btr2))\n",
        "#btr=np.transpose(btr)\n",
        "#print(btr.shape)\n",
        "Bsr=np.hstack((Bsr3,Bsr1,Bsr2))\n",
        "#bsr=np.transpose(bsr)\n",
        "B=np.vstack((Btr,Bsr))\n",
        "B.shape\n"
      ],
      "metadata": {
        "id": "GvvrTNcJLyrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d880af9-4040-4e3a-9dae-52bde7a808af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 823
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "Btotal=np.hstack((B,total))\n",
        "Btotal.shape\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ],
      "metadata": {
        "id": "cTOu75kxMXlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(Btotal)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,label)"
      ],
      "metadata": {
        "id": "7I-HR4rMMeHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2b0578-29c8-406c-9c64-69b5d6809796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6191"
            ]
          },
          "metadata": {},
          "execution_count": 825
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "_6wl5gL5fVqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cddb4b7-a3ec-4e3a-ca61-d1ab187dab0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6199999468010312"
            ]
          },
          "metadata": {},
          "execution_count": 826
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "xJPzqc9zMg4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f0f07f-b5e2-4730-da74-0fbe6c9a9f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23999989360206225"
            ]
          },
          "metadata": {},
          "execution_count": 827
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "1ZrNLsJF-BIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Translation Attack"
      ],
      "metadata": {
        "id": "QmCCKiVCn1QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def translate_image(x1,d):  #d=distance\n",
        "  x2=np.zeros(x1.shape)\n",
        "  x3=np.zeros(x1.shape)\n",
        "  x4=np.zeros(x1.shape)\n",
        "  x5=np.zeros(x1.shape)\n",
        "  x6=np.zeros(x1.shape)\n",
        "  tp=x1.dtype\n",
        "  x2=x2.astype(tp)\n",
        "  x3=x3.astype(tp)\n",
        "  x4=x4.astype(tp)\n",
        "  x5=x5.astype(tp)\n",
        "  x6=x6.astype(tp)\n",
        "  for i in range(len(x1)):\n",
        "    test_x=x1[i]\n",
        "    i1=1#random.uniform(-d,d)\n",
        "    j1=d-abs(i1)\n",
        "    out1=ndimage.interpolation.shift(test_x,np.array([i1,j1,0]))\n",
        "    #print(i1,j1)\n",
        "    i2=-1#random.uniform(-d,d)\n",
        "    j2=d-abs(i2)\n",
        "    out2=ndimage.interpolation.shift(test_x,np.array([i2,j2,0]))\n",
        "    i3=0#random.uniform(-d,d)\n",
        "    j3=d-abs(i3)\n",
        "    out3=ndimage.interpolation.shift(test_x,np.array([i3,j3,0]))\n",
        "    i4=0#random.uniform(-d,d)\n",
        "    j4=-1#d-abs(i4)\n",
        "    out4=ndimage.interpolation.shift(test_x,np.array([i4,j4,0]))\n",
        "    i5=0#random.uniform(-d,d)\n",
        "    j5=0#d-abs(i5)\n",
        "    out5=ndimage.interpolation.shift(test_x,np.array([i5,j5,0]))\n",
        "    #_____________________________________________#\n",
        "    x2[i]=out1\n",
        "    x3[i]=out2\n",
        "    x4[i]=out3\n",
        "    x5[i]=out4\n",
        "    x6[i]=out5\n",
        "  return x2,x3,x4,x5,x6"
      ],
      "metadata": {
        "id": "kUzrEGQxn3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_trans1,train_data_trans2,train_data_trans3,train_data_trans4,train_data_trans5=translate_image(train_data[5000:10000],1)\n",
        "test_data_trans1,test_data_trans2,test_data_trans3,test_data_trans4,test_data_trans5=translate_image(test_data[0:5000],1)"
      ],
      "metadata": {
        "id": "FRcSKXSvoPxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(target_model,data,t):\n",
        "  n=len(data)\n",
        "  ytr=target_model.predict(data)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "0m-jm_21tpUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=train_labels[5000:10000].reshape(5000,1)\n",
        "s=test_labels[0:5000].reshape(5000,1)\n",
        "T=train_data[5000:10000].reshape(5000,32*32*3)\n",
        "S=test_data[0:5000].reshape(5000,32*32*3)"
      ],
      "metadata": {
        "id": "gOajorOhuEU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Calculate features for training data')\n",
        "btr1=calc_b(target_model,train_data_trans1,t)\n",
        "btr2=calc_b(target_model,train_data_trans2,t)\n",
        "btr3=calc_b(target_model,train_data_trans3,t)\n",
        "btr4=calc_b(target_model,train_data_trans4,t)\n",
        "btr5=calc_b(target_model,train_data_trans5,t)\n",
        "#btr6=calc_b(model_np,train_data[5000:10000],t)\n",
        "\n",
        "print('Calculate features for testing data')\n",
        "bsr1=calc_b(target_model,test_data_trans1,s)\n",
        "bsr2=calc_b(target_model,test_data_trans2,s)\n",
        "bsr3=calc_b(target_model,test_data_trans3,s)\n",
        "bsr4=calc_b(target_model,test_data_trans4,s)\n",
        "bsr5=calc_b(target_model,test_data_trans5,s)\n",
        "#bsr6=calc_b(model_np,test_data[0:5000],s)\n",
        "\n",
        "print('rearrange b')\n",
        "btr=np.hstack((btr5,btr1,btr2,btr3,btr4))\n",
        "#btr=np.transpose(btr)\n",
        "#print(btr.shape)\n",
        "bsr=np.hstack((bsr5,bsr1,bsr2,bsr3,bsr4))\n",
        "#bsr=np.transpose(bsr)\n",
        "b=np.vstack((btr,bsr))\n",
        "b.shape"
      ],
      "metadata": {
        "id": "CGpkxkkuuQfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0682f26-6dbf-48ca-b7c2-344be5fb23a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate features for training data\n",
            "Calculate features for testing data\n",
            "rearrange b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 833
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "B=np.hstack((b,total))\n",
        "B.shape"
      ],
      "metadata": {
        "id": "JXmQfHwRum8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ae600a-ef09-4c7f-9c17-16fa71b8357a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 834
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1=np.ones(5000)\n",
        "out2=np.zeros(5000)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ],
      "metadata": {
        "id": "uMKSjyE2urYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd12fd2-e8a3-476f-e389-49ebd99c8f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "t_data,s_data,t_label,s_label=train_test_split(B, label, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "lfOSOhU5uvJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train a model\n",
        "modelLB=make_fc()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# compile the model\n",
        "modelLB.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "# train the model\n",
        "history = modelLB.fit(t_data, t_label,\n",
        "                       validation_data=(s_data, s_label),\n",
        "                       batch_size=32, \n",
        "                       epochs=30)"
      ],
      "metadata": {
        "id": "HS1cIw9du0e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(s_data)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,s_label)"
      ],
      "metadata": {
        "id": "o6WIFQmiu_An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, s_label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "-IeqNu1_vD5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "p0qh5PRSvJrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!!"
      ],
      "metadata": {
        "id": "P4pWEsFu-g07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_B(target_model,data,t):    #if only MemGuard\n",
        "  n=len(data)\n",
        "  ytr=memguard(data)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "fp0FNxXEAHdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_B(private_model,data,t):    #if only model_stacking\n",
        "  n=len(data)\n",
        "  ytr=model_stacking(data,private_model)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "pDShmjQ9exbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_B(target_model,data,t):    #if only PATE\n",
        "  n=len(data)\n",
        "  ytr=calc_pate(data,ep[0])\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "TRNFwJXEGBcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_B(private_model,data,t):    \n",
        "  n=len(data)\n",
        "  ytr=private_model.predict(data)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr"
      ],
      "metadata": {
        "id": "iiUkHgT397tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Private Model for TA attack\n",
        "train_data_trans1,train_data_trans2,train_data_trans3,train_data_trans4,train_data_trans5=translate_image(train_data[0:5000],1)\n",
        "test_data_trans1,test_data_trans2,test_data_trans3,test_data_trans4,test_data_trans5=translate_image(test_data[5000:10000],1)"
      ],
      "metadata": {
        "id": "tekQtPsD-qQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=train_labels[0:5000].reshape(5000,1)\n",
        "s=test_labels[5000:10000].reshape(5000,1)\n",
        "T=train_data[0:5000].reshape(5000,32*32*3)\n",
        "S=test_data[5000:10000].reshape(5000,32*32*3)"
      ],
      "metadata": {
        "id": "VUXLxx5f_9Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Calculate features for training data')\n",
        "import timeit\n",
        "print('Calculate features for training data')\n",
        "btr1=calc_B(private_model,train_data_trans1,t)\n",
        "btr2=calc_B(private_model,train_data_trans2,t)\n",
        "btr3=calc_B(private_model,train_data_trans3,t)\n",
        "btr4=calc_B(private_model,train_data_trans4,t)\n",
        "btr5=calc_B(private_model,train_data_trans5,t)\n",
        "#btr6=calc_b(model_np,train_data[5000:10000],t)\n",
        "\n",
        "print('Calculate features for testing data')\n",
        "bsr1=calc_B(private_model,test_data_trans1,s)\n",
        "bsr2=calc_B(private_model,test_data_trans2,s)\n",
        "bsr3=calc_B(private_model,test_data_trans3,s)\n",
        "bsr4=calc_B(private_model,test_data_trans4,s)\n",
        "bsr5=calc_B(private_model,test_data_trans5,s)\n",
        "#bsr6=calc_b(model_np,test_data[0:5000],s)\n",
        "\n",
        "print('rearrange b')\n",
        "btr=np.hstack((btr5,btr1,btr2,btr3,btr4))\n",
        "#btr=np.transpose(btr)\n",
        "#print(btr.shape)\n",
        "bsr=np.hstack((bsr5,bsr1,bsr2,bsr3,bsr4))\n",
        "#bsr=np.transpose(bsr)\n",
        "b1=np.vstack((btr,bsr))\n",
        "b1.shape"
      ],
      "metadata": {
        "id": "8Fx6xxsRAAmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44b81a7-9afc-4754-e91a-9cda9f74db80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate features for training data\n",
            "Calculate features for testing data\n",
            "rearrange b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 854
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "Btotal=np.hstack((b1,total))\n",
        "Btotal.shape"
      ],
      "metadata": {
        "id": "sA2Z_cMLCxYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76cb77b9-3eb4-4056-bae9-f34d17ce1139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 855
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(Btotal)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,label)"
      ],
      "metadata": {
        "id": "Ou111wL3C1Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "wxFvOttvC3L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "8F5l171sC5MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "yPU5phGbC6_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boundary Distance Attack"
      ],
      "metadata": {
        "id": "gzaEw6dExNbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n1=5000"
      ],
      "metadata": {
        "id": "zGNV3yfOxMtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_curve"
      ],
      "metadata": {
        "id": "6gIUrIi9xf0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def continuous_rand_robust(model, dsx,dsy, max_samples=100, noise_samples=250, stddev=0.025, input_dim=shape,\n",
        "                           num=[1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 150, 250]): #350, 500, 750, 1000, 1500, 2000, 2500]):\n",
        "  \"\"\"Calculate robustness to random noise for Adv-x MI attack on continuous-featureed datasets (+ UCI adult).\n",
        "  :param model: model to approximate distances on (attack).\n",
        "  :param ds: tf dataset should be either the training set or the test set.\n",
        "  :param max_samples: maximum number of samples to take from the ds\n",
        "  :param noise_samples: number of noised samples to take for each sample in the ds.\n",
        "  :param stddev: the standard deviation to use for Gaussian noise (only for Adult, which has some continuous features)\n",
        "  :param input_dim: dimension of inputs for the dataset.\n",
        "  :param num: subnumber of samples to evaluate. max number is noise_samples\n",
        "  :return: a list of lists. each sublist of the accuracy on up to $num noise_samples.\n",
        "  \"\"\"\n",
        "  robust_accs = [[] for _ in num]\n",
        "  iter=len(dsx)\n",
        "  labels=dsy\n",
        "  for i in range(iter):\n",
        "    x=dsx[i:i+1]\n",
        "    noise = stddev * np.random.randn(noise_samples, 32,32,3)\n",
        "    x_noisy = np.clip(x + noise, 0, 1)\n",
        "    #print(noise.shape)\n",
        "    prob=model.predict(x_noisy)\n",
        "    pred=np.argmax(prob,axis=1)\n",
        "    #print(pred.shape)\n",
        "    for idx, n in enumerate(num):\n",
        "            if n == 0:\n",
        "              robust_accs[idx].append(1)\n",
        "            else:\n",
        "              robust_accs[idx].append(np.mean(pred[:n] == labels[i]))\n",
        "  return robust_accs"
      ],
      "metadata": {
        "id": "FMaj5ADZxi9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def continuous_rand_robust_target(model, dsx,dsy, max_samples=100, noise_samples=250, stddev=0.025, input_dim=shape,\n",
        "                           num=[1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 150, 250]): #350, 500, 750, 1000, 1500, 2000, 2500]):\n",
        "  \"\"\"Calculate robustness to random noise for Adv-x MI attack on continuous-featureed datasets (+ UCI adult).\n",
        "  :param model: model to approximate distances on (attack).\n",
        "  :param ds: tf dataset should be either the training set or the test set.\n",
        "  :param max_samples: maximum number of samples to take from the ds\n",
        "  :param noise_samples: number of noised samples to take for each sample in the ds.\n",
        "  :param stddev: the standard deviation to use for Gaussian noise (only for Adult, which has some continuous features)\n",
        "  :param input_dim: dimension of inputs for the dataset.\n",
        "  :param num: subnumber of samples to evaluate. max number is noise_samples\n",
        "  :return: a list of lists. each sublist of the accuracy on up to $num noise_samples.\n",
        "  \"\"\"\n",
        "  robust_accs = [[] for _ in num]\n",
        "  iter=len(dsx)\n",
        "  labels=dsy\n",
        "  for i in range(iter):\n",
        "    x=dsx[i:i+1]\n",
        "    noise = stddev * np.random.randn(noise_samples, 32,32,3)\n",
        "    x_noisy = np.clip(x + noise, 0, 1)\n",
        "    #print(noise.shape)\n",
        "    #prob=memguard(x_noisy)    #if MemGuard\n",
        "    #prob=calc_pate(x_noisy,ep[0])  #if PATE\n",
        "    prob=model.predict(x_noisy)  #others\n",
        "    #prob=model_stacking(x_noisy,model)  #model stacking\n",
        "    pred=np.argmax(prob,axis=1)\n",
        "    #print(pred.shape)\n",
        "    for idx, n in enumerate(num):\n",
        "            if n == 0:\n",
        "              robust_accs[idx].append(1)\n",
        "            else:\n",
        "              robust_accs[idx].append(np.mean(pred[:n] == labels[i]))\n",
        "  return robust_accs"
      ],
      "metadata": {
        "id": "aGlwI8FmD256"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_source_in=continuous_rand_robust(target_model,train_data[0:n1],train_labels[0:n1])\n",
        "noise_source_out=continuous_rand_robust(target_model,test_data[0:n1],test_labels[0:n1])"
      ],
      "metadata": {
        "id": "q46VZg-sxnTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_target_in=continuous_rand_robust_target(private_model,train_data[n1:2*n1],train_labels[n1:2*n1])\n",
        "noise_target_out=continuous_rand_robust_target(private_model,test_data[n1:2*n1],test_labels[n1:2*n1])"
      ],
      "metadata": {
        "id": "Xy-EKt6Uxto8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_m = np.concatenate([np.ones(n1),np.zeros(n1)], axis=0)\n",
        "target_m = np.concatenate([np.ones(n1),np.zeros(n1)], axis=0)"
      ],
      "metadata": {
        "id": "2uy1AkIVx7XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_accuracy(y_true, probs, thresholds=None):\n",
        "  \"\"\"Return the max accuracy possible given the correct labels and guesses. Will try all thresholds unless passed.\n",
        "  Args:\n",
        "    y_true: True label of `in' or `out' (member or non-member, 1/0)\n",
        "    probs: The scalar to threshold\n",
        "    thresholds: In a blackbox setup with a shadow/source model, the threshold obtained by the source model can be passed\n",
        "      here for attackin the target model. This threshold will then be used.\n",
        "  Returns: max accuracy possible, accuracy at the threshold passed (if one was passed), the max precision possible,\n",
        "   and the precision at the threshold passed.\n",
        "  \"\"\"\n",
        "  attack_adv=[]\n",
        "  if thresholds is None:\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
        "    attack_adv.append(abs(tpr-fpr))\n",
        "    \n",
        "\n",
        "  accuracy_scores = []\n",
        "  precision_scores = []\n",
        "  for thresh in thresholds:\n",
        "    accuracy_scores.append(accuracy_score(y_true,\n",
        "                                          [1 if m > thresh else 0 for m in probs]))\n",
        "    precision_scores.append(precision_score(y_true, [1 if m > thresh else 0 for m in probs]))\n",
        "\n",
        "  accuracies = np.array(accuracy_scores)\n",
        "  precisions = np.array(precision_scores)\n",
        "  attack_adv=np.array(attack_adv)\n",
        "  max_accuracy = accuracies.max()\n",
        "  max_precision = precisions.max()\n",
        "  #print(type(attack_adv))\n",
        "  #print(attack_adv)\n",
        "  if (attack_adv.size !=0):\n",
        "      max_adv=np.max(attack_adv)\n",
        "      print(max_adv)\n",
        "  max_accuracy_threshold = thresholds[accuracies.argmax()]\n",
        "  max_precision_threshold = thresholds[precisions.argmax()]\n",
        "  return max_accuracy, max_accuracy_threshold, max_precision, max_precision_threshold"
      ],
      "metadata": {
        "id": "zaU9J3NVx_qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_threshold(source_m, source_stats, target_m, target_stats):\n",
        "  \"\"\" Train a threshold attack model and get teh accuracy on source and target models.\n",
        "  Args:\n",
        "    source_m: membership labels for source dataset (1 for member, 0 for non-member)\n",
        "    source_stats: scalar values to threshold (attack features) for source dataset\n",
        "    target_m: membership labels for target dataset (1 for member, 0 for non-member)\n",
        "    target_stats: scalar values to threshold (attack features) for target dataset\n",
        "  Returns: best acc from source thresh, precision @ same threshold, threshold for best acc,\n",
        "    precision at the best threshold for precision. all tuned on source model.\n",
        "  \"\"\"\n",
        "  # find best threshold on source data\n",
        "  acc_source, t, prec_source, tprec = get_max_accuracy(source_m, source_stats)\n",
        "\n",
        "  # find best accuracy on test data (just to check how much we overfit)\n",
        "  acc_test, _, prec_test, _ = get_max_accuracy(target_m, target_stats)\n",
        "\n",
        "  # get the test accuracy at the threshold selected on the source data\n",
        "  acc_test_t, _, _, _ = get_max_accuracy(target_m, target_stats, thresholds=[t])\n",
        "  _, _, prec_test_t, _ = get_max_accuracy(target_m, target_stats, thresholds=[tprec])\n",
        "  print(\"acc src: {}, acc test (best thresh): {}, acc test (src thresh): {}, thresh: {}\".format(acc_source, acc_test,\n",
        "                                                                                                acc_test_t, t))\n",
        "  print(\n",
        "    \"prec src: {}, prec test (best thresh): {}, prec test (src thresh): {}, thresh: {}\".format(prec_source, prec_test,\n",
        "                                                                                               prec_test_t, tprec))\n",
        "\n",
        "  return acc_test_t, prec_test_t, t, tprec"
      ],
      "metadata": {
        "id": "cHDFgzVzyMfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(noise_source_in)):\n",
        "            noise_source = np.concatenate([noise_source_in[i], noise_source_out[i]], axis=0)\n",
        "            noise_target = np.concatenate([noise_target_in[i], noise_target_out[i]], axis=0)\n",
        "            get_threshold(source_m, noise_source, target_m, noise_target)"
      ],
      "metadata": {
        "id": "3wyJOanXyTRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "YSJeHK9GXhTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cifar10_and_Cifar100_Related_Work.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}