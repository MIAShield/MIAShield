{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MIAShield/MIAShield/blob/main/Evaluation_of_MIAShield_(Cifar10_and_100).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmdcapYQOv26"
      },
      "outputs": [],
      "source": [
        "#install all required library\n",
        "!sudo pip install Pillow\n",
        "!sudo pip install imagehash\n",
        "!pip install tensorflow_privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCHWQehucWLX"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m3tMXxJZLVD"
      },
      "outputs": [],
      "source": [
        "#!pip install opencv-python-headless==4.5.4.60\n",
        "#!pip install fiftyone --no-binary fiftyone,voxel51-eta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBSlmSTAPF0-"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import glob\n",
        "import itertools\n",
        "import collections\n",
        "#import fiftyone as fo\n",
        "#import fiftyone.zoo as foz\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import imagehash\n",
        "import hashlib\n",
        "\n",
        "\n",
        "#importing libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# general imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy import special\n",
        "\n",
        "# tensorflow imports\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZk4t_kOPoZ9",
        "outputId": "b2542621-2d05-4548-b98d-95fbb26816e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_privacy/privacy/membership_inference_attack/__init__.py:19: UserWarning: \n",
            "Membership inference attack sources were moved. Please replace\n",
            "import tensorflow_privacy.privacy.membership_inference_attack\n",
            "\n",
            "with\n",
            "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack\n",
            "  \"\\nMembership inference attack sources were moved. Please replace\"\n"
          ]
        }
      ],
      "source": [
        "# tensorflow-privacy libraries\n",
        "from tensorflow_privacy.privacy.membership_inference_attack import membership_inference_attack as mia\n",
        "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import AttackInputData\n",
        "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import SlicingSpec\n",
        "from tensorflow_privacy.privacy.membership_inference_attack.data_structures import AttackType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUaRffzGPzEA"
      },
      "source": [
        "Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPNKHpHyP0j1"
      },
      "outputs": [],
      "source": [
        "dataset='Cifar10'            #dataset, 'Cifar10'/'Cifar100'\n",
        "EO='EO2'                     #EO: 'EO2','EO3', 'EO4', 'EO5'\n",
        "batch_size=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arnh_Ql5P5cB"
      },
      "outputs": [],
      "source": [
        "if dataset=='Cifar10':\n",
        "  #define all the variables\n",
        "#cifar10\n",
        "  shape = (32, 32, 3)\n",
        "  num_class=10\n",
        "  learning_rate=.01\n",
        "  epochs=60\n",
        "  M=5\n",
        "elif dataset=='Cifar100':\n",
        "  #cifar100\n",
        "  shape = (32, 32, 3)\n",
        "  num_class=100\n",
        "  learning_rate=.01\n",
        "  epochs=130\n",
        "  M=4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyGTnL7dP_LW"
      },
      "outputs": [],
      "source": [
        "def load_cifar10():\n",
        "  \"\"\"Loads cifar10-Dataset and preprocesses to combine training and test data.\"\"\"\n",
        "  \n",
        "  # load the existing CIFAR10 dataset that comes in form of traing + test data and labels\n",
        "  train, test = tf.keras.datasets.cifar10.load_data()\n",
        "  train_data, train_labels = train\n",
        "  test_data, test_labels = test\n",
        "\n",
        "  # scale the images from color values 0-255 to numbers from 0-1 to help the training process\n",
        "  train_data1 = np.array(train_data, dtype=np.float32) / 255\n",
        "  test_data1 = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "  # cifar10 labels come one-hot encoded, there\n",
        "  train_labels = train_labels.flatten()\n",
        "  test_labels = test_labels.flatten()\n",
        "\n",
        "  return train_data1, train_labels, test_data1, test_labels, train_data, train_labels, test_data, test_labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu1tFTKZQCCT"
      },
      "outputs": [],
      "source": [
        "def load_cifar100():\n",
        "  \"\"\"Loads cifar100-Dataset and preprocesses to combine training and test data.\"\"\"\n",
        "  \n",
        "  # load the existing CIFAR10 dataset that comes in form of traing + test data and labels\n",
        "  train, test = tf.keras.datasets.cifar100.load_data()\n",
        "  train_data, train_labels = train\n",
        "  test_data, test_labels = test\n",
        "\n",
        "  # scale the images from color values 0-255 to numbers from 0-1 to help the training process\n",
        "  train_data1 = np.array(train_data, dtype=np.float32) / 255\n",
        "  test_data1 = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "  # cifar10 labels come one-hot encoded, there\n",
        "  train_labels = train_labels.flatten()\n",
        "  test_labels = test_labels.flatten()\n",
        "\n",
        "  return train_data1, train_labels, test_data1, test_labels, train_data, train_labels, test_data, test_labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGy8it2oQD-2",
        "outputId": "c68a45c7-d31c-44fc-ecb9-90891bffcef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ],
      "source": [
        "if dataset=='Cifar10':\n",
        "       train_data, train_labels, test_data, test_labels,trainX1,trainY1, testX1,testY1 = load_cifar10()  \n",
        "elif dataset=='Cifar100':\n",
        "       train_data, train_labels, test_data, test_labels,trainX1,trainY1, testX1,testY1 = load_cifar100() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5cMtA4gQGsz"
      },
      "outputs": [],
      "source": [
        "train_y=to_categorical(train_labels)\n",
        "test_y=to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN-ZKmQYQJE4"
      },
      "outputs": [],
      "source": [
        "def AlexnetModel(input_shape,num_classes):\n",
        "    model = Sequential()\n",
        "# model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
        "# for original Alexnet\n",
        "    model.add(Conv2D(48, (3,3), strides=(2,2), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(96, (3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "# Local Response normalization for Original Alexnet\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Functions"
      ],
      "metadata": {
        "id": "eUiKrHS6FjnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return final"
      ],
      "metadata": {
        "id": "bslSrO5xFlJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return labels"
      ],
      "metadata": {
        "id": "OboS9HYGFuh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVwrfPrtQR8Y"
      },
      "source": [
        "Non-Private Model_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEVtsXgAQLUc"
      },
      "outputs": [],
      "source": [
        "# make the neural network model with the function specified above.\n",
        "# one model is supposed to train for 10, one for 50 epochs\n",
        "model_np = AlexnetModel(shape,num_class)\n",
        "model_np.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOd4eQgdQWDW"
      },
      "outputs": [],
      "source": [
        "# specify parameters\n",
        "#learning_rate=.005\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=.01)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# compile the model\n",
        "model_np.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqvIDPGjQfgS"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "history = model_np.fit(train_data, train_y,\n",
        "                       validation_data=(test_data, test_y),\n",
        "                       batch_size=128, \n",
        "                       epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVyKZCZIQgRv",
        "outputId": "96f8b0b3-19ae-4d48-899f-80a75982f5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict on train...\n",
            "Predict on test...\n"
          ]
        }
      ],
      "source": [
        "# since we have not specified an activation function on the last layer\n",
        "# calling the predict function returns the logits\n",
        "print('Predict on train...')\n",
        "prob_train = model_np.predict(train_data[0:5000])\n",
        "print('Predict on test...')\n",
        "prob_test = model_np.predict(test_data[5000:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEfi5fHdQi-Z"
      },
      "outputs": [],
      "source": [
        "train=train_data[0:5000]\n",
        "trainX=trainX1[0:5000]\n",
        "test=test_data[5000:10000]\n",
        "testX=testX1[5000:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_71EKqRCBh"
      },
      "outputs": [],
      "source": [
        "stop!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJgD3HWFREIZ"
      },
      "source": [
        "Training MIAShield Models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqvuSGHNRDUD"
      },
      "outputs": [],
      "source": [
        "#split datasets\n",
        "#from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "#ros = RandomOverSampler()\n",
        "\n",
        "#X_ros, y_ros = ros.fit_resample(x_train_split0, trainy_split0)\n",
        "# split x_train to 5 disjoint datasets\n",
        "#store each dataset variable name is the list Xtrain\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "X=train_data\n",
        "Y=train_labels\n",
        "for i in range(0,M):\n",
        "  K=M-i\n",
        "  if(K>=2):\n",
        "    train=X\n",
        "    test=Y\n",
        "    skf = StratifiedKFold(n_splits=K)\n",
        "    skf.get_n_splits(X, Y)\n",
        "    #print(skf)\n",
        "    for train_index, test_index in skf.split(X, Y):\n",
        "       #print(X.shape)\n",
        "       X,globals()['x_train_split%s' % i]=train[train_index], train[test_index]\n",
        "       Y,globals()['trainy_split%s' % i]=test[train_index], test[test_index]\n",
        "       y_hot=globals()['trainy_split%s' % i]\n",
        "       y_hot=to_categorical(y_hot)\n",
        "       globals()['y_train_split%s' % i]=y_hot\n",
        "       #X=train\n",
        "       #Y=test\n",
        "  elif K==1:\n",
        "    globals()['x_train_split%s' % i]=X\n",
        "    globals()['trainy_split%s' % i]=Y\n",
        "    y_hot=to_categorical(Y)\n",
        "    globals()['y_train_split%s' % i]=y_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK5Gg1W0UKGT"
      },
      "outputs": [],
      "source": [
        "#create datagenerator for augmented training\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "width_shift = 3/32\n",
        "height_shift = 3/32\n",
        "flip = True\n",
        "rotation=10\n",
        "datagen = ImageDataGenerator(\n",
        "    horizontal_flip=flip,\n",
        "    width_shift_range=.1,\n",
        "    height_shift_range=.1,\n",
        "    #brightness_range=[0.02,.1],\n",
        "    rotation_range=rotation,\n",
        "    #zoom_range=[0.5,1.0]\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeOG3EUTUVAN"
      },
      "outputs": [],
      "source": [
        "for iter in range(0,M):\n",
        "  modelt=AlexnetModel(shape,num_class)\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=.01)\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "# compile the model\n",
        "  modelt.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  #create augmented data\n",
        "  data_t=globals()['x_train_split%s' % iter]\n",
        "  target_t=globals()['y_train_split%s' % iter]\n",
        "  dataGen=datagen\n",
        "  dataGen.fit(data_t)\n",
        "  #train=datagen.flow(data_t, target_t, batch_size=128)\n",
        "# Fit the model\n",
        "  globals()['history_%s' % iter] = modelt.fit_generator(dataGen.flow(data_t, target_t, batch_size=128, shuffle=False),\n",
        "                    steps_per_epoch = 10000 / 128, epochs=epochs+30, validation_data=(test_data, test_y))\n",
        "  globals()['model_%s' % iter]=modelt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MesqEOzBBKNg",
        "outputId": "4e0cf371-7e97-431c-e160-a9e847de7d86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5842"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#calculate accuracy/utility test acc\n",
        "prb=model_0.predict(test_data[0:5000])\n",
        "test_labeld_result=np.argmax(prb,axis=1)\n",
        "accuracy_score(test_labels[0:5000],test_labeld_result)\n",
        "#prob_testd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWyJCrn3iQoM",
        "outputId": "0a7a9a13-cdb6-4adf-8928-ea905fd386c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "4\n",
            "3\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "#from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "#ros = RandomOverSampler()\n",
        "\n",
        "#X_ros, y_ros = ros.fit_resample(x_train_split0, trainy_split0)\n",
        "# split x_train to 5 disjoint datasets\n",
        "#store each dataset variable name is the list Xtrain\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "X=trainX1\n",
        "Y=trainY1\n",
        "for i in range(0,M):\n",
        "  K=M-i\n",
        "  if(K>=2):\n",
        "    train=X\n",
        "    test=Y\n",
        "    skf = StratifiedKFold(n_splits=K)\n",
        "    print(skf.get_n_splits(X, Y))\n",
        "    for train_index, test_index in skf.split(X, Y):\n",
        "       X,globals()['x_traind%s' % i]=train[train_index], train[test_index]\n",
        "    #print(X.shape)\n",
        "       Y,globals()['y_traind%s' % i]=test[train_index], test[test_index]\n",
        "  elif K==1:\n",
        "    globals()['x_traind%s' % i]=X\n",
        "    globals()['y_traind%s' % i]=Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI4iF25Bi6Zz",
        "outputId": "c68acd8a-d235-4970-a71c-1555d5ce3da6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 5., 5., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#target model index\n",
        "in1=np.zeros(5000)\n",
        "in2=M*np.ones(5000)\n",
        "index_target=np.hstack((in1,in2))\n",
        "index_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVwBnQYfp7Rh"
      },
      "outputs": [],
      "source": [
        "stop!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I8vnqRpQ7eQ"
      },
      "source": [
        "Exclusion Oracle-2: Hash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Qkqdueh6oN"
      },
      "outputs": [],
      "source": [
        "def convert_hash(trainX1):\n",
        "  lenth=len(trainX1)\n",
        "  hashid=np.zeros(lenth)\n",
        "  hash=[]\n",
        "  for id in range(lenth):\n",
        "    hashid[id]=id\n",
        "    #covert in hashes\n",
        "    #im_array=Image.fromarray(trainX1[id].astype('uint8'), 'RGB')\n",
        "    im_array=trainX1[id]\n",
        "    hash1 = int(hashlib.sha1(bytearray(im_array)).hexdigest(), 16) % (10 ** 8)\n",
        "    #hash1=np.array(hash1)\n",
        "    #print(type(hash1))\n",
        "    #print(np.array(hash1))\n",
        "    hash.append(hash1)\n",
        "  return np.array(hash)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ImageHash\n",
        "def convert_hash(trainX1):\n",
        "  lenth=len(trainX1)\n",
        "  hashid=np.zeros(lenth)\n",
        "  hash=[]\n",
        "  for id in range(lenth):\n",
        "    hashid[id]=id\n",
        "    #covert in hashes\n",
        "    im_array=Image.fromarray(trainX1[id].astype('uint8'), 'RGB')\n",
        "    #im_array=trainX1[id]\n",
        "    hash1 = imagehash.phash(im_array)\n",
        "    #print(type(hash1))\n",
        "    hash1=str(hash1)\n",
        "    hash1=int(hash1,16)\n",
        "    #print(type(hash1))\n",
        "    #print(np.array(hash1))\n",
        "    hash.append(hash1)\n",
        "  return np.array(hash)"
      ],
      "metadata": {
        "id": "LR9d9LvhFMt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYdKSsCliGQv"
      },
      "outputs": [],
      "source": [
        "def BinarySearch(lys, val):\n",
        "    first = 0\n",
        "    last = len(lys)-1\n",
        "    index = -1\n",
        "    while (first <= last) and (index == -1):\n",
        "        mid = (first+last)//2\n",
        "        if lys[mid] == val:\n",
        "            index = mid\n",
        "        else:\n",
        "            if val<lys[mid]:\n",
        "                last = mid -1\n",
        "            else:\n",
        "                first = mid +1\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-ACURSNiLi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cc6cd7-1255-4510-914b-dc2ed5a8b658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9224074046236128083  9227865158289475454  9229351712225673679 ...\n",
            " 18443510138377475449 18445794649772851073 18446656124593965524]\n",
            "[ 9224717234222054331  9227153475723306698  9231285630047250339 ...\n",
            " 18443999699996328761 18445304129594335692 18446137253901635920]\n",
            "[ 9227257094460995507  9227372289600986061  9229913402847975376 ...\n",
            " 18444660049367318630 18445268674639339917 18445656393653657218]\n",
            "[ 9225482563983374288  9225500275768108443  9229898577180112369 ...\n",
            " 18444086560571320528 18445741535015244957 18446571519321713862]\n",
            "[ 9224282938412692095  9224844555567814501  9225446168794521979 ...\n",
            " 18445588323256274944 18446322422092191532 18446682228339522112]\n"
          ]
        }
      ],
      "source": [
        "#calculate and store hashes for M=5 #different model and their training dataset\n",
        "for x in range(0,M):\n",
        "             #print(j,k+j)\n",
        "             x1=globals()['x_traind%s' % x]\n",
        "             x2=convert_hash(x1)\n",
        "             globals()['x_hash%s' % x]=np.sort(x2)\n",
        "             print(globals()['x_hash%s' % x])            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx9L_-uJihuy"
      },
      "outputs": [],
      "source": [
        "def find_duplicate(x1,xhash):\n",
        "  hash1=convert_hash(x1)\n",
        "  value=BinarySearch(xhash,hash1)\n",
        "  if(value!=-1):\n",
        "      mem=1\n",
        "  elif(value==-1):\n",
        "      mem=0\n",
        "  return mem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0iUXXhqirqU"
      },
      "outputs": [],
      "source": [
        "#check which model contain the example\n",
        "def model_index_EO2(d1,d2,M):\n",
        "  for i1 in range(M):\n",
        "    traind=globals()['x_hash%s' % i1]\n",
        "    #traind=shuffle(traind1)\n",
        "    #traind=np.sort(traind)\n",
        "    member=find_duplicate(d2,traind)\n",
        "    if (member==1):\n",
        "      index=i1\n",
        "      break\n",
        "  if (member==0):\n",
        "    index=M\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "K2Bt53cy-3Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwIu2QXN1xG3"
      },
      "source": [
        "Exclusion Oracle-3: ImageHash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3ct3fkq10Kx"
      },
      "outputs": [],
      "source": [
        "def convert_Imhash(trainX1):\n",
        "  lenth=len(trainX1)\n",
        "  hashid=np.zeros(lenth)\n",
        "  hash=[]\n",
        "  for id in range(lenth):\n",
        "    hashid[id]=id\n",
        "    #covert in hashes\n",
        "    im_array=Image.fromarray(np.uint8(trainX1[id])).convert('RGB')\n",
        "    #im_array=Image.fromarray(trainX1[id].astype('uint8'), 'RGB')\n",
        "    #im_array=trainX1[id]\n",
        "    hash1 = imagehash.phash(im_array)\n",
        "    #print(type(hash1))\n",
        "    #hash1=str(hash1)\n",
        "    #hash1=int(hash1,16)\n",
        "    #print(type(hash1))\n",
        "    #print(np.array(hash1))\n",
        "    hash.append(hash1)\n",
        "  return np.array(hash)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feDSMbNM2zrt"
      },
      "outputs": [],
      "source": [
        "def hamming_dist(h1,h2):\n",
        "  n=len(h2)\n",
        "  d=np.zeros(n)\n",
        "  for i in range(n):\n",
        "    d[i]=h1-h2[i]\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5VhdfWJ2-Ms"
      },
      "outputs": [],
      "source": [
        "#check which model contain the example\n",
        "def model_index_EO3(d1,d2,M):\n",
        "  for i1 in range(M):\n",
        "    traind=globals()['x_hash%s' % i1]\n",
        "    #traind=shuffle(traind1)\n",
        "    #traind=np.sort(traind)\n",
        "    h1=convert_Imhash(d2)\n",
        "    d=hamming_dist(h1,traind)\n",
        "    member=np.count_nonzero(d < 14)\n",
        "    #print(member)\n",
        "    if (member>=1):\n",
        "      index=i1\n",
        "      break\n",
        "  if (member==0):\n",
        "    index=M\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx9CtnKy3CDq"
      },
      "outputs": [],
      "source": [
        "#calculate and store hashes for M=5 #different model and their training dataset\n",
        "#Only for imagehash\n",
        "for x in range(0,M):\n",
        "             #print(j,k+j)\n",
        "             x1=globals()['x_traind%s' % x]\n",
        "             x2=convert_Imhash(x1)\n",
        "             globals()['x_hash%s' % x]=x2\n",
        "             #print(globals()['x_hash%s' % x])\n",
        "             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dReSnTqukrpL"
      },
      "source": [
        "Exclusion Oracle-4:Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXXsNuK-l6Mc"
      },
      "outputs": [],
      "source": [
        "d_len=2500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM7vqU-AkuSk"
      },
      "outputs": [],
      "source": [
        "#PCA\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.decomposition import PCA\n",
        "def get_PCA(xin,a):\n",
        "    d0,d1,d2,d3=xin.shape\n",
        "    xin=xin.reshape(d1*d0,d2*d3)\n",
        "    xpca=xin\n",
        "    pca = PCA(n_components=a) #a=number of components that we want to consider\n",
        "    #pca=PCA(.5)\n",
        "    fit = pca.fit(xpca)\n",
        "    f=fit.components_\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSVaaRLClhv_"
      },
      "outputs": [],
      "source": [
        "def Overfit(model,x_test,Y_test):\n",
        "  y_p=model.predict(x_test)\n",
        "  y_p=y_p.reshape(num_class)\n",
        "  #print(y_p)\n",
        "  #print(Y_test)\n",
        "  i3=np.argmax(Y_test)\n",
        "  out=y_p[i3]\n",
        "  #y_p=label(y_p)\n",
        "  #print(y_p)\n",
        "  y_t=label(Y_test)\n",
        "  #print(y_t)\n",
        "  #out=y_p[y_t]\n",
        "  #out=accuracy_score(y_t,y_p)*100\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bYsbEDTlmh6"
      },
      "outputs": [],
      "source": [
        "def Confidence_Vector(model,train,label):\n",
        "  logit=model.predict(train)\n",
        "  #print(logit)\n",
        "  prob= special.softmax(logit, axis=1)\n",
        "  prob=prob.reshape(num_class,1)\n",
        "  out=prob[label]\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdaT8OI1lwXj"
      },
      "outputs": [],
      "source": [
        "num2=d_len #subset training data from each model\n",
        "if dataset=='Cifar10':\n",
        "  d=np.concatenate((np.zeros(num2), np.ones(num2),np.ones(num2)*2,np.ones(num2)*3,np.ones(num2)*4))\n",
        "elif dataset=='Cifar100':\n",
        "  d=np.concatenate((np.zeros(num2), np.ones(num2),np.ones(num2)*2,np.ones(num2)*3))\n",
        "\n",
        "d.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea8MYjetmDyI"
      },
      "outputs": [],
      "source": [
        "def MIA_classM(nd,iter):\n",
        "  mdt3=M #total Number of model\n",
        "  mt3=nd #Total number of samples per model \n",
        "  m13=1  #number of different dataset participate\n",
        "  datat3=np.zeros([mdt3*mt3*m13,M]) \n",
        "  datadt3=np.zeros([mdt3*mt3*m13,32*3*3])  #taking input image\n",
        "  dataxt3=np.zeros([mdt3*mt3*m13,1]) \n",
        "  dataf=np.zeros([mdt3*mt3*m13,512]) #label\n",
        "  ##create a dataset data1[samples,modelno,prediction_vectors]\n",
        "#datax=[]\n",
        "  k1=0\n",
        "  for i in range(0,iter):\n",
        "  #m=500\n",
        "    x1=globals()['x_train_split%s' % i]\n",
        "    y1=globals()['trainy_split%s' % i]\n",
        "     #m=no. of samples, md=prediction of models , last=MNIST 10 prediction values\n",
        "    for b1 in range(mt3):\n",
        "       x2=x1[b1:b1+1]\n",
        "       y2=y1[b1:b1+1]\n",
        "    #print(x2.shape)\n",
        "       ypred=np.zeros([mdt3])\n",
        "       for b2 in range(mdt3):\n",
        "         modelc=globals()['model_%s' % b2]\n",
        "       #modelc.predict_proba(x2)\n",
        "         yc=Confidence_Vector(modelc,x2,y2)\n",
        "         ypred[b2]=yc\n",
        "       #xd=x2.reshape(28*28)\n",
        "     #Parallel(n_jobs=5, require='sharedmem')(delayed(indexM)(b2) for b2 in range(md))\n",
        "       xpca=get_PCA(x2,3)\n",
        "       d1,d2=xpca.shape\n",
        "       xd=xpca.reshape(d1*d2)\n",
        "       xr=y2\n",
        "       #xf=to_feature_maps(x2)\n",
        "       #xf=xf.reshape(512*1*1*1)\n",
        "       dataxt3[k1,:]=xr\n",
        "       datadt3[k1,:]=xd\n",
        "       #dataf[k1,:]=xf\n",
        "     #p=ypred[b2,:]\n",
        "     #xp=[xr,p]\n",
        "       #datak.append(xp)   #take the input vector as an array\n",
        "       datat3[k1,:]=ypred\n",
        "       k1=k1+1\n",
        "  arrt3=np.hstack((datadt3,datat3,dataxt3))\n",
        "  return arrt3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63HSx_AZpjBJ"
      },
      "outputs": [],
      "source": [
        "arr1=MIA_classM(d_len,M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuK0dSqqEGyD"
      },
      "outputs": [],
      "source": [
        "Y=d\n",
        "X=arr1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1onm3O5tELO5"
      },
      "outputs": [],
      "source": [
        "#split the dataset into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(X, Y, test_size=0.00001, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPwE44M4GFgk"
      },
      "outputs": [],
      "source": [
        "#build a model for logistic Regression \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "modelL = RandomForestClassifier(max_depth=20, random_state=42)\n",
        "#modelL=svm.SVC()\n",
        "#modelL = tree.DecisionTreeClassifier()\n",
        "modelL.fit(X_train_ex,y_train_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYV54BSGGJca"
      },
      "outputs": [],
      "source": [
        "#Threshold value\n",
        "def predict_label(yp1,t):\n",
        "  val=np.max(yp1)\n",
        "  if (val>=t):\n",
        "    lb=np.argmax(yp1)\n",
        "  else:\n",
        "    lb=M\n",
        "  return lb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Wm7pQFL65_"
      },
      "outputs": [],
      "source": [
        "def MIA_class(dataM,label,nd):\n",
        "  iter=1\n",
        "  mdt3=M #total Number of model\n",
        "  mt3=nd #Total number of samples per model \n",
        "  m13=1  #number of different dataset participate\n",
        "  datat3=np.zeros([mt3*m13,M]) \n",
        "  datadt3=np.zeros([mt3*m13,32*3*3])  #taking input image\n",
        "  dataxt3=np.zeros([mt3*m13,1]) \n",
        "  dataf=np.zeros([mt3*m13,512]) #label\n",
        "  ##create a dataset data1[samples,modelno,prediction_vectors]\n",
        "#datax=[]\n",
        "  k1=0\n",
        "  for i in range(0,iter):\n",
        "  #m=500\n",
        "    x1=dataM#globals()['x_train_split%s' % i]\n",
        "    y1=label#globals()['trainy_split%s' % i]\n",
        "     #m=no. of samples, md=prediction of models , last=MNIST 10 prediction values\n",
        "    for b1 in range(mt3):\n",
        "       x2=x1[b1:b1+1]\n",
        "       y2=y1[b1:b1+1]\n",
        "    #print(x2.shape)\n",
        "       ypred=np.zeros([mdt3])\n",
        "       for b2 in range(mdt3):\n",
        "         modelc=globals()['model_%s' % b2]\n",
        "       #modelc.predict_proba(x2)\n",
        "         yc=Confidence_Vector(modelc,x2,y2)\n",
        "         ypred[b2]=yc\n",
        "       #xd=x2.reshape(28*28)\n",
        "     #Parallel(n_jobs=5, require='sharedmem')(delayed(indexM)(b2) for b2 in range(md))\n",
        "       xpca=get_PCA(x2,3)\n",
        "       d1,d2=xpca.shape\n",
        "       xd=xpca.reshape(d1*d2)\n",
        "       xr=y2\n",
        "       #xf=to_feature_maps(x2)\n",
        "       #xf=xf.reshape(512*1*1*1)\n",
        "       dataxt3[k1,:]=xr\n",
        "       datadt3[k1,:]=xd\n",
        "       #dataf[k1,:]=xf\n",
        "     #p=ypred[b2,:]\n",
        "     #xp=[xr,p]\n",
        "       #datak.append(xp)   #take the input vector as an array\n",
        "       datat3[k1,:]=ypred\n",
        "       k1=k1+1\n",
        "  arrt3=np.hstack((datadt3,datat3,dataxt3))\n",
        "  return arrt3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_uLdbU4WDTu"
      },
      "outputs": [],
      "source": [
        "def MIA_classNM(dataM,nd):\n",
        "  mdt3=M #total Number of model\n",
        "  mt3=nd #Total number of samples per model \n",
        "  m13=1  #number of different dataset participate\n",
        "  datat3=np.zeros([mt3*m13,M]) \n",
        "  datadt3=np.zeros([mt3*m13,32*3*3])  #taking input image\n",
        "  dataxt3=np.zeros([mt3*m13,1]) #label\n",
        "  dataf=np.zeros([mdt3*mt3*m13,512])  #feature mapping\n",
        "  ##create a dataset data1[samples,modelno,prediction_vectors]\n",
        "#datax=[]\n",
        "  k1=0\n",
        "  for i in range(0,1):\n",
        "  #m=500\n",
        "    x1=dataM\n",
        "    #y1=data_label\n",
        "     #m=no. of samples, md=prediction of models , last=MNIST 10 prediction values\n",
        "    for b1 in range(mt3):\n",
        "       x2=x1[b1:b1+1]\n",
        "       #y2=y1[b1:b1+1]\n",
        "    #print(x2.shape)\n",
        "       ypred=np.zeros([mdt3])\n",
        "       for b2 in range(mdt3):\n",
        "         modelc=globals()['model_%s' % b2]\n",
        "       #modelc.predict_proba(x2)\n",
        "         y2=modelc.predict(x2)\n",
        "         y2=np.argmax(y2)\n",
        "         yc=Confidence_Vector(modelc,x2,y2)\n",
        "         ypred[b2]=yc\n",
        "       #xd=x2.reshape(28*28)\n",
        "     #Parallel(n_jobs=5, require='sharedmem')(delayed(indexM)(b2) for b2 in range(md))\n",
        "       xpca=get_PCA(x2,3)\n",
        "       d1,d2=xpca.shape\n",
        "       xd=xpca.reshape(d1*d2)\n",
        "       xr=y2\n",
        "       #xf=to_feature_maps(x2)\n",
        "       #xf=xf.reshape(512*1*1*1)\n",
        "       dataxt3[k1,:]=xr\n",
        "       datadt3[k1,:]=xd\n",
        "       #dataf[k1,:]=xf\n",
        "     #p=ypred[b2,:]\n",
        "     #xp=[xr,p]\n",
        "       #datak.append(xp)   #take the input vector as an array\n",
        "       datat3[k1,:]=ypred\n",
        "       k1=k1+1\n",
        "  arrt3=np.hstack((datadt3,datat3,dataxt3))\n",
        "  return arrt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrPd2sOjMdn8"
      },
      "outputs": [],
      "source": [
        "arr2=MIA_class(test_data[0:5000],test_labels[0:5000],5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zwjmgWuTJlm"
      },
      "outputs": [],
      "source": [
        "arr=np.vstack((arr1,arr2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N9qc1MJTYhq"
      },
      "outputs": [],
      "source": [
        "numt=5000#subset training data from each model\n",
        "dt=(np.ones(numt)*M)\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfqVrDtsTNp6"
      },
      "outputs": [],
      "source": [
        "d=d.reshape(len(d),1)\n",
        "dt=dt.reshape(len(dt),1)\n",
        "d_total=np.vstack((d,dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kidI6EaLTbt1"
      },
      "outputs": [],
      "source": [
        "#final Train-Test set for thresholding\n",
        "Xtest=arr\n",
        "Ytest=d_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Piw8wga_Tlpl"
      },
      "outputs": [],
      "source": [
        "y_pred=modelL.predict_proba(Xtest)\n",
        "#model_index=label(y_pred)\n",
        "#accuracy_score(Ytest,y_pred)*100\n",
        "#model_index=model_index.reshape(1000,6)\n",
        "#model_index\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkfChnjJTtTP"
      },
      "outputs": [],
      "source": [
        "th=np.linspace(0,1,20)\n",
        "y_acc1=np.zeros(len(th))\n",
        "for j in range(len(th)):\n",
        "  ylab=np.zeros(len(y_pred))\n",
        "  for i in range(len(y_pred)):\n",
        "     ylab[i]=predict_label(y_pred[i,:],th[j])\n",
        "\n",
        "  y_acc1[j]=accuracy_score(Ytest,ylab)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H7gI49ZTwKE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "plt.plot(th,y_acc1,color=\"blue\",marker='o',linewidth=2.0, linestyle=\"--\",label='X_pca=288')\n",
        "#plt.plot(th,y_acc,color=\"blue\",marker='+',linewidth=1.5, linestyle=\"-\",label='n_component=1')\n",
        "plt.legend(fontsize=12)\n",
        "plt.xlabel(\"Threshold (th)\",fontsize=15)\n",
        "plt.ylabel(\"Decision Accuracy\",fontsize=15)\n",
        "test1.show()\n",
        "test1.set_facecolor('white')\n",
        "test1.savefig('Acc_Vs_threshold_cifar10-5.pdf')\n",
        "#files.download('Acc_Vs_threshold_cifar10-5.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yVaczNdT250"
      },
      "outputs": [],
      "source": [
        "inx=np.argmax(y_acc1)\n",
        "thres=th[inx+1]\n",
        "thres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJjZSFNDT9PZ"
      },
      "outputs": [],
      "source": [
        "y_acc1[inx+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY3sSBe-wGU9"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def calcl_correlation():\n",
        "  x1=get_PCA(im1,3)\n",
        "  x1=x1.reshape(1,32*3*3)\n",
        "  x2=get_PCA(im2,3)\n",
        "  x2=x2.reshape(1,32*3*3)\n",
        "  dist=cosine_similarity(x1,x2)\n",
        "  return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_C8gK1SUA5H"
      },
      "outputs": [],
      "source": [
        "def model_index_EO4(d1,d2,M):\n",
        "  arr_test=MIA_classNM(d1,len(d1))\n",
        "  ypt=modelL.predict_proba(arr_test)\n",
        "  indx=predict_label(ypt,thres)\n",
        "  return indx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTlMGBpoUyw1"
      },
      "source": [
        "Exclusion Oracle-5: Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u3KkspSU2YV"
      },
      "outputs": [],
      "source": [
        "def model_index_EO5(d1,d2,M,lab):\n",
        "  index=model_index_EO2(d2,M)\n",
        "  if index==M:\n",
        "    index=model_index_EO3(d2,M)\n",
        "    if index==M:\n",
        "      index=model_index_EO4(d1,M)\n",
        "    else:\n",
        "      index=index\n",
        "  else:\n",
        "    index=index\n",
        "  return index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TaHjABBmSlC"
      },
      "source": [
        "transformation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx1LTGbhmUkG"
      },
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "def rotate_image(x1,a):  #angle\n",
        "  x2=np.zeros(x1.shape)\n",
        "  x3=np.zeros(x1.shape)\n",
        "  tp=x1.dtype\n",
        "  x2=x2.astype(tp)\n",
        "  x3=x3.astype(tp)\n",
        "  for i in range(len(x1)):\n",
        "    out1=ndimage.rotate(x1[i],a,reshape=False)\n",
        "    #out2=ndimage.rotate(x1[i],-a,reshape=False)\n",
        "    x2[i]=out1\n",
        "    #x3[i]=out2\n",
        "  return x2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CK8BmuSjFx3"
      },
      "source": [
        "calculate the EO accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxb1IAql771"
      },
      "outputs": [],
      "source": [
        "dn=5000\n",
        "train1=x_train_split0[0:dn]\n",
        "trainx1=x_traind0[0:dn]\n",
        "test1=test_data[5000:5000+dn]\n",
        "testx1=testX1[5000:5000+dn]\n",
        "r=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_UODYjQnES_"
      },
      "outputs": [],
      "source": [
        "XT=x_train_split0[0:dn]\n",
        "XT1=x_traind0[0:dn]\n",
        "TestX=test_data[5000:10000]\n",
        "TestX1=testX1[5000:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxRpMcNOmKuY"
      },
      "outputs": [],
      "source": [
        "#rotation\n",
        "train_data_rotate=rotate_image(XT,r)\n",
        "trainX_data_rotate=rotate_image(XT1,r)\n",
        "test_data_rotate=rotate_image(TestX,r)\n",
        "testX_data_rotate=rotate_image(TestX1,r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5jpHrWpjUV_"
      },
      "source": [
        "start evaluating PD attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_Z9m1HHl48N"
      },
      "outputs": [],
      "source": [
        "#define input\n",
        "train_d=train1\n",
        "train_x=trainx1\n",
        "test_d=test1\n",
        "test_x=testx1\n",
        "yt=trainy_split0[0:dn]\n",
        "ys=test_labels[5000:5000+dn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e2BYLqgizgN"
      },
      "outputs": [],
      "source": [
        "#calculate parameters from our technique: Only Model index for EO accuracy\n",
        "#from joblib import Parallel, delayed\n",
        "import time\n",
        "def calc_EO_acc(data1,data2):\n",
        "  if EO=='EO2':\n",
        "    model_index=model_index_EO2\n",
        "  elif EO=='EO3':\n",
        "    model_index=model_index_EO3\n",
        "  elif EO=='EO4':\n",
        "    model_index=model_index_EO4\n",
        "  elif EO=='EO5':\n",
        "    model_index=model_index_EO5\n",
        "  \n",
        "  n=len(data1)\n",
        "  logits=np.zeros((n,num_class))\n",
        "  prob=np.zeros((n,num_class))\n",
        "  labels=np.zeros(n)\n",
        "  index_resulted=np.zeros(n)\n",
        "  ts=.02\n",
        "  for p1 in range(n):\n",
        "    kk=0\n",
        "    d1=data1[p1:p1+1]\n",
        "    d2=data2[p1:p1+1]\n",
        "    #sum1=np.zeros(10)\n",
        "    sum2=np.zeros(num_class)\n",
        "    index=model_index(d1,d2,M)\n",
        "    index_resulted[p1]=index\n",
        "  return index_resulted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1s2p3aT6ef1"
      },
      "outputs": [],
      "source": [
        "#calculate parameters from our combined\n",
        "#from joblib import Parallel, delayed\n",
        "import time\n",
        "import timeit\n",
        "def calc_attack_matrix(data1,data2):\n",
        "  n=len(data1)\n",
        "  logits=np.zeros((n,num_class))\n",
        "  prob=np.zeros((n,num_class))\n",
        "  labels=np.zeros(n)\n",
        "  index_resulted=np.zeros(n)\n",
        "  index_arr=np.zeros((n,293))\n",
        "  ts=.02\n",
        "  kk=0\n",
        "  #Define Model Index\n",
        "  if EO=='EO2':\n",
        "    model_index=model_index_EO2\n",
        "  elif EO=='EO3':\n",
        "    model_index=model_index_EO3\n",
        "  elif EO=='EO4':\n",
        "    model_index=model_index_EO4\n",
        "  elif EO=='EO5':\n",
        "    model_index=model_index_EO5\n",
        "  for p1 in range(n):\n",
        "    d1=data1[p1:p1+1]\n",
        "    d2=data2[p1:p1+1]\n",
        "    #sum1=np.zeros(10)\n",
        "    sum2=np.zeros(num_class)\n",
        "    index=model_index(d1,d2,M)\n",
        "    #print(arr_t.shape)\n",
        "    #index_arr[p1,:]=arr_t\n",
        "    index_resulted[p1]=index\n",
        "    #print(index)\n",
        "    for p2 in range(M):\n",
        "      if(index!=p2):\n",
        "        start = timeit.default_timer()\n",
        "        modelh=globals()['model_%s' % p2]\n",
        "        prob_d = modelh.predict(d1)\n",
        "        #prob_d = special.softmax(logits_d)\n",
        "        kk=kk+1\n",
        "        #sum1=logits_d+sum1\n",
        "        prob_vote=confidence(prob_d)\n",
        "        sum2=prob_d+sum2\n",
        "        #print(sum1)\n",
        "        #print(sum2)\n",
        "        stop = timeit.default_timer()\n",
        "        ts=stop-start\n",
        "      else:\n",
        "        time.sleep(ts)\n",
        "    #logits[p1,:]=sum1/100\n",
        "    #print(logits)\n",
        "    prob[p1,:]=sum2/kk*2\n",
        "    #labels[p1,:]=np.argmax(logits[p1,:])\n",
        "  return prob,index_resulted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2XPZwRWjmWy"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "print('Predict on train...')\n",
        "start1 = timeit.default_timer()\n",
        "#logits_traind,prob_traind,train_index = calc_attack_matrix(train_data[0:5000],trainX1[0:5000])\n",
        "prob_traind,train_index = calc_attack_matrix(train_d,train_x)\n",
        "stop1 = timeit.default_timer()\n",
        "print('time required for members',stop1-start1)\n",
        "print('Predict on test...')\n",
        "start2 = timeit.default_timer()\n",
        "prob_testd,test_index = calc_attack_matrix(test_d,test_x)\n",
        "stop2 = timeit.default_timer()\n",
        "print('time required for non-members',stop2-start2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT5i4csm9bGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235386f5-9c89-407f-b474-69546fb5c817"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 5., 5., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#target model index\n",
        "in1=np.zeros(dn)\n",
        "in2=M*np.ones(dn)\n",
        "index_target=np.hstack((in1,in2))\n",
        "index_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d93DplGZk5Dd"
      },
      "outputs": [],
      "source": [
        "result_index=np.hstack((train_index,test_index))\n",
        "print(result_index)\n",
        "accuracy_score(index_target,result_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfHSvxCE_ZM0"
      },
      "outputs": [],
      "source": [
        "(unique, counts) = np.unique(train_index, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggWBW9RDExDT"
      },
      "source": [
        "Probability Dependent Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ2L-SYFEzrS"
      },
      "outputs": [],
      "source": [
        "prob_trainf=prob_traind\n",
        "prob_testf=prob_testd\n",
        "yt=trainy_split0[0:dn]\n",
        "ys=test_labels[5000:5000+dn]\n",
        "attack=1      #define attacks. if 1-Th, 2-LR, 3-MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MmTXjCZFE5E"
      },
      "outputs": [],
      "source": [
        "# define what variables our attacker should have access to\n",
        "attack_input = AttackInputData(\n",
        "  logits_train = prob_trainf,\n",
        "  logits_test = prob_testf,\n",
        "  #loss_train = loss_train,\n",
        "  #loss_test = loss_test,\n",
        "  labels_train =yt,\n",
        "  labels_test =ys\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8ogporVFILR"
      },
      "outputs": [],
      "source": [
        "# how should the data be sliced\n",
        "slicing_spec = SlicingSpec(\n",
        "    entire_dataset = True,\n",
        "    by_class = False,\n",
        "    by_percentiles = False,\n",
        "    by_classification_correctness = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03f-bknIFKmA"
      },
      "outputs": [],
      "source": [
        "# define the type of attacker model that we want to use\n",
        "if attack==1:\n",
        "  attack_types = [\n",
        "    AttackType.THRESHOLD_ATTACK\n",
        "]\n",
        "elif attack==2:\n",
        "  attack_types = [\n",
        "    AttackType.LOGISTIC_REGRESSION\n",
        "]\n",
        "elif attack==3:\n",
        "  attack_types = [\n",
        "    AttackType.MULTI_LAYERED_PERCEPTRON\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i7d6fsDFNPb"
      },
      "outputs": [],
      "source": [
        "# run the attack\n",
        "attacks_result = mia.run_attacks(attack_input=attack_input,\n",
        "                                 slicing_spec=slicing_spec,\n",
        "                                 attack_types=attack_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYOeBc8lFQE5"
      },
      "outputs": [],
      "source": [
        "# summary by data slice (the best performing attacks per slice are presented)\n",
        "print(attacks_result.summary(by_slices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuDyAibDFSdP"
      },
      "outputs": [],
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"calculate model utility\")\n",
        "test_labeld_result=np.argmax(prob_testf,axis=1)\n",
        "accuracy_score(ys,test_labeld_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d70UEIpcFXLp"
      },
      "outputs": [],
      "source": [
        "#calculate train accuracy/utility\n",
        "print(\"Calculate Member's accuracy\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "train_labeld_result=np.argmax(prob_trainf,axis=1)\n",
        "accuracy_score(yt,train_labeld_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCgv0AhvFiK_"
      },
      "source": [
        "GAP Attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJrGihIpFhdb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgpQQnSdFm0t"
      },
      "outputs": [],
      "source": [
        "in1=np.zeros(dn)\n",
        "in2=np.ones(dn)\n",
        "Attack_target=np.hstack((in2,in1))\n",
        "Attack_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyTOtqvUFo3D"
      },
      "outputs": [],
      "source": [
        "def Gap_attack(prob_train,prob_test,label_train,label_test):\n",
        "  result1=np.argmax(prob_train,axis=1)\n",
        "  result2=np.argmax(prob_test,axis=1)\n",
        "  result_label=np.hstack((result1,result2))\n",
        "  target_label=np.hstack((label_train,label_test))\n",
        "  target_label=np.array(target_label)\n",
        "  g1=len(target_label)\n",
        "  target_label=target_label.reshape(g1,1)\n",
        "  result_label=np.array(result_label)\n",
        "  result_label=result_label.reshape(g1,1)\n",
        "  target_label=target_label.astype(int)\n",
        "  result_label=result_label.astype(int)\n",
        "  attack_result=np.equal(target_label, result_label)\n",
        "  attack_result=attack_result.astype(int) \n",
        "  fpr, tpr, thresholds = metrics.roc_curve(Attack_target, attack_result, pos_label=1)\n",
        "  AUC=metrics.auc(fpr, tpr)\n",
        "  advantage=np.max(abs(fpr-tpr))\n",
        "  return AUC, advantage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wiQkz6-Fq_p"
      },
      "outputs": [],
      "source": [
        "#gap attack for non-private data\n",
        "auc,adv=Gap_attack(prob_trainf,prob_testf,yt,ys)\n",
        "print(auc)\n",
        "print(adv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DW4DeSLFuIv"
      },
      "outputs": [],
      "source": [
        "stop!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label-Only Attack:Initialization"
      ],
      "metadata": {
        "id": "Ee0885XQTzRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage, misc\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nTnVY9HfT1ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the flags\n",
        "r=15   #1-15\n",
        "target_model=model_np \n",
        "private_model=model_np#the model that needs to be evaluated\n",
        "train=train_data[5000:10000]\n",
        "test=test_data[0:5000]"
      ],
      "metadata": {
        "id": "I8_voSc9UDCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_fc():\n",
        "  \"\"\"Return conv_model as tf.keras.Model, without Softmax layer (only logits).\n",
        "  To perform\n",
        "  :param input_shape:\n",
        "  :param depth:\n",
        "  :param regularization:\n",
        "  :param reg_constant:\n",
        "  :param num_classes:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  fc_model = tf.keras.models.Sequential()\n",
        "  #fc_model.add(tf.keras.layers.Dense(num_classes, kernel_regularizer=k_reg))\n",
        "  fc_model.add(tf.keras.layers.Dense(10, activation=tf.keras.layers.ReLU(\n",
        "        negative_slope=1e-2), kernel_initializer='glorot_normal'))\n",
        "  fc_model.add(keras.layers.Dense(10, activation=tf.keras.layers.ReLU(\n",
        "        negative_slope=1e-2), kernel_initializer='glorot_normal'))\n",
        "  fc_model.add(tf.keras.layers.Dense(2, kernel_initializer='glorot_normal'))\n",
        "  fc_model.add(tf.keras.layers.Softmax())\n",
        "  return fc_model"
      ],
      "metadata": {
        "id": "4SIeeL0mUGR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RA attack"
      ],
      "metadata": {
        "id": "mNdxUycJGY2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_image(x1,a):  #angle\n",
        "  x2=np.zeros(x1.shape)\n",
        "  x3=np.zeros(x1.shape)\n",
        "  tp=x1.dtype\n",
        "  x2=x2.astype(tp)\n",
        "  x3=x3.astype(tp)\n",
        "  for i in range(len(x1)):\n",
        "    out1=ndimage.rotate(x1[i],a,reshape=False)\n",
        "    out2=ndimage.rotate(x1[i],-a,reshape=False)\n",
        "    x2[i]=out1\n",
        "    x3[i]=out2\n",
        "  return x2,x3"
      ],
      "metadata": {
        "id": "uuZ5Q4ihUIyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_rotate1,train_data_rotate2=rotate_image(train_data[5000:10000],r)\n",
        "test_data_rotate1,test_data_rotate2=rotate_image(test_data[0:5000],r)"
      ],
      "metadata": {
        "id": "-z7b3cGFUMBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(target_model,data,t):\n",
        "  n=len(data)\n",
        "  ytr=target_model.predict(data)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr,ytr"
      ],
      "metadata": {
        "id": "tcgAjv3lUWQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=train_labels[5000:10000].reshape(5000,1)\n",
        "s=test_labels[0:5000].reshape(5000,1)\n",
        "T=train_data[5000:10000].reshape(5000,32*32*3)\n",
        "S=test_data[0:5000].reshape(5000,32*32*3)"
      ],
      "metadata": {
        "id": "jg33itUAUZ5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate b\n",
        "btr1,ytr1=calc_b(target_model,train_data_rotate1,t)\n",
        "btr2,ytr2=calc_b(target_model,train_data_rotate2,t)\n",
        "btr3,ytr3=calc_b(target_model,train,t)\n",
        "\n",
        "\n",
        "bsr1,ysr1=calc_b(target_model,test_data_rotate1,s)\n",
        "bsr2,ysr2=calc_b(target_model,test_data_rotate2,s)\n",
        "bsr3,ysr3=calc_b(target_model,test,s)\n",
        "\n",
        "out1=np.ones(5000)\n",
        "out2=np.zeros(5000)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "#rearrange b\n",
        "btr=np.hstack((btr3,btr1,btr2))\n",
        "#btr=np.transpose(btr)\n",
        "#print(btr.shape)\n",
        "bsr=np.hstack((bsr3,bsr1,bsr2))\n",
        "#bsr=np.transpose(bsr)\n",
        "b=np.vstack((btr,bsr))\n",
        "b.shape"
      ],
      "metadata": {
        "id": "b7dGegCPUcTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "Bt=np.hstack((b,total))\n",
        "Bt.shape\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ],
      "metadata": {
        "id": "w_jIpfjeUhIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "t_data,s_data,t_label,s_label=train_test_split(Bt, label, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "mBsWViG8UlER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train a model\n",
        "modelLB=make_fc()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# compile the model\n",
        "modelLB.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "# train the model\n",
        "history = modelLB.fit(t_data, t_label,\n",
        "                       validation_data=(s_data, s_label),\n",
        "                       batch_size=32, \n",
        "                       epochs=30)"
      ],
      "metadata": {
        "id": "a-yHF7BdUns0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(s_data)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,s_label)"
      ],
      "metadata": {
        "id": "bqzydEI2UsJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, s_label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "6Kdi8xHEUvZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "cZA5lB5RUxxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "accuracy_score(t,ytr1)"
      ],
      "metadata": {
        "id": "QQ9EFgRBuj__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "accuracy_score(s,ysr1)"
      ],
      "metadata": {
        "id": "5uPmbdPTuwPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "A0hQ6FrxrqqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Private Model"
      ],
      "metadata": {
        "id": "DNqH3TgNVOfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(private_model,data1,data2,t):    #if only model_stacking\n",
        "  n=len(data1)\n",
        "  ytr,indx=calc_attack_matrix(data1,data2)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  return btr,ytr"
      ],
      "metadata": {
        "id": "Jx-euCIwU0I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create training and testing rotate data\n",
        "train_data_rotateT1,train_data_rotateT2=rotate_image(train1,r)\n",
        "train_X1_rotateT1,train_X1_rotateT2=rotate_image(trainx1,r)\n",
        "test_data_rotateT1,test_data_rotateT2=rotate_image(test1,r)\n",
        "test_X1_rotateT1,test_X1_rotateT2=rotate_image(testx1,r)"
      ],
      "metadata": {
        "id": "d4Vvy74vVNwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=yt.reshape(dn,1)\n",
        "s=ys.reshape(dn,1)\n",
        "T=train1.reshape(dn,32*32*3)\n",
        "S=test1.reshape(dn,32*32*3)"
      ],
      "metadata": {
        "id": "5noTNv4ZVl2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate b\n",
        "Btr1,Ytr1=calc_b(private_model,train_data_rotateT1,train_X1_rotateT1,t)\n",
        "Btr2,Ytr2=calc_b(private_model,train_data_rotateT2,train_X1_rotateT2,t)\n",
        "Btr3,Ytr3=calc_b(private_model,train1,trainx1,t)\n",
        "\n",
        "Bsr1,Ysr1=calc_b(private_model,test_data_rotateT1,test_X1_rotateT1,s)\n",
        "Bsr2,Ysr2=calc_b(private_model,test_data_rotateT2,test_X1_rotateT2,s)\n",
        "Bsr3,Ysr3=calc_b(private_model,test1,testx1,s)\n",
        "\n",
        "out1=np.ones(dn)\n",
        "out2=np.zeros(dn)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "#rearrange b\n",
        "Btr=np.hstack((Btr3,Btr1,Btr2))\n",
        "#btr=np.transpose(btr)\n",
        "#print(btr.shape)\n",
        "Bsr=np.hstack((Bsr3,Bsr1,Bsr2))\n",
        "#bsr=np.transpose(bsr)\n",
        "B=np.vstack((Btr,Bsr))\n",
        "B.shape"
      ],
      "metadata": {
        "id": "S-wxsBkCV3yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "Btotal=np.hstack((B,total))\n",
        "Btotal.shape\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ],
      "metadata": {
        "id": "slZQxW9QW7A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(Btotal)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,label)"
      ],
      "metadata": {
        "id": "4Dl2wwgsW7z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "GXv5mzDNW-EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "pKkCuD-5XAfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "accuracy_score(t,Ytr1)"
      ],
      "metadata": {
        "id": "tsKvohMw_PjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "accuracy_score(s,Ysr1)"
      ],
      "metadata": {
        "id": "51Tp9TdD_QT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "dCoflo1LUf5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA attack"
      ],
      "metadata": {
        "id": "NC4uT8QUWBiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#d=1\n",
        "N=5\n",
        "#d=np.array([0,1,-1,1,-1]) \n",
        "#d=np.array([0,3,2,1,-3,-2,-1,3,2,1,-3,-2,-1])  #d=3\n",
        "d=np.array([0,5,4,3,2,1,-5,-4,-3,-2,-1,5,4,3,2,1,-5,-4,-3,-2,-1])\n",
        "kv=np.ceil(len(d)/2)"
      ],
      "metadata": {
        "id": "cITiRlIAmqzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def translate_images1(x1,d):  #d=distance\n",
        "  x2=np.zeros(x1.shape)\n",
        "  x3=np.zeros(x1.shape)\n",
        "  tp=x1.dtype\n",
        "  x2=x2.astype(tp)\n",
        "  x3=x3.astype(tp)\n",
        "  for i in range(len(x1)):\n",
        "    test_x=x1[i]\n",
        "    if d==0:\n",
        "      i1=0\n",
        "      j1=0\n",
        "    else:\n",
        "      i1=d#random.uniform(-d,d)\n",
        "      j1=N-abs(i1)\n",
        "\n",
        "    out1=ndimage.interpolation.shift(test_x,np.array([i1,j1,0,]))\n",
        "    #print(i1,j1)\n",
        "    #i2=0#random.uniform(-d,d)\n",
        "    #j2=d\n",
        "    #out2=ndimage.interpolation.shift(test_x,np.array([i2,j2,0]))\n",
        "    \n",
        "    #_____________________________________________#\n",
        "    x2[i]=out1\n",
        "    \n",
        "\n",
        "  return x2"
      ],
      "metadata": {
        "id": "yuDwKH3VWEPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def translate_images2(x1,d):  #d=distance\n",
        "  x2=np.zeros(x1.shape)\n",
        "  x3=np.zeros(x1.shape)\n",
        "  tp=x1.dtype\n",
        "  x2=x2.astype(tp)\n",
        "  x3=x3.astype(tp)\n",
        "  for i in range(len(x1)):\n",
        "    test_x=x1[i]\n",
        "    j1=d#random.uniform(-d,d)\n",
        "    i1=N-abs(j1)\n",
        "    out1=ndimage.interpolation.shift(test_x,np.array([i1,j1,0]))\n",
        "    #print(i1,j1)\n",
        "    #i2=0#random.uniform(-d,d)\n",
        "    #j2=d\n",
        "    #out2=ndimage.interpolation.shift(test_x,np.array([i2,j2,0]))\n",
        "    \n",
        "    #_____________________________________________#\n",
        "    x2[i]=out1\n",
        "    \n",
        "\n",
        "  return x2"
      ],
      "metadata": {
        "id": "552sPPuffYlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_b(target_model,data,t,d,k):\n",
        "  n=len(data)\n",
        "  if k>=kv:\n",
        "    translate_images=translate_images2\n",
        "  else:\n",
        "    translate_images=translate_images1\n",
        "  datat=translate_images(data,d)\n",
        "  ytr=target_model.predict(datat)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  ytr=ytr.reshape(len(ytr))\n",
        "  btr=btr.reshape(len(btr))\n",
        "  return btr,ytr"
      ],
      "metadata": {
        "id": "Zt4Ga3nhWlan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=train_labels[5000:10000].reshape(5000,1)\n",
        "s=test_labels[0:5000].reshape(5000,1)\n",
        "T=train_data[5000:10000].reshape(5000,32*32*3)\n",
        "S=test_data[0:5000].reshape(5000,32*32*3)"
      ],
      "metadata": {
        "id": "Uyn2e-JwWnzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "btr=np.zeros((len(t),len(d)))\n",
        "ytr=np.zeros((len(t),len(d)))\n",
        "#indxt=np.zeros(len(t),len(d))\n",
        "print('Calculate features for training data')\n",
        "for i in range(len(d)):\n",
        "  btr[:,i],ytr[:,i]=calc_b(target_model,train_data[5000:10000],t,d[i],i)\n",
        "\n",
        "print('Calculate features for testing data')\n",
        "bsr=np.zeros((len(t),len(d)))\n",
        "ysr=np.zeros((len(t),len(d)))\n",
        "#indxt=np.zeros(len(t),len(d))\n",
        "for i in range(len(d)):\n",
        "  bsr[:,i],ysr[:,i]=calc_b(target_model,test_data[0:5000],s,d[i],i)\n",
        "\n",
        "\n",
        "print('rearrange b')\n",
        "b=np.vstack((btr,bsr))\n",
        "b.shape"
      ],
      "metadata": {
        "id": "v91qhTI8Wra_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "B=np.hstack((b,total))\n",
        "B.shape"
      ],
      "metadata": {
        "id": "bS8Zzp_bWuZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out1=np.ones(dn)\n",
        "out2=np.zeros(dn)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ],
      "metadata": {
        "id": "F017ukRAWx5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce286e9d-d6ba-4a3f-ab5e-c0c7479ca269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "t_data,s_data,t_label,s_label=train_test_split(B, label, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "rwbLNMvYW0KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train a model\n",
        "modelLB=make_fc()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# compile the model\n",
        "modelLB.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "# train the model\n",
        "history = modelLB.fit(t_data, t_label,\n",
        "                       validation_data=(s_data, s_label),\n",
        "                       batch_size=32, \n",
        "                       epochs=30)"
      ],
      "metadata": {
        "id": "HQIOmCnBW3OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(s_data)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,s_label)"
      ],
      "metadata": {
        "id": "Tqgyt_cwW6lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, s_label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "WKqrCXR4W9BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "Wr-lzhwXW_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "#acct=accuracy_score(t,ytr1)+accuracy_score(t,ytr2)+accuracy_score(t,ytr3)+accuracy_score(t,ytr4)+accuracy_score(t,ytr5)\n",
        "#print(acct/5)\n",
        "acct=np.zeros(len(d))\n",
        "for i in range(len(d)):\n",
        "  acct[i]=accuracy_score(t,ytr[:,i])\n",
        "\n",
        "print(np.average(acct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyolRQKrkkQ6",
        "outputId": "0f0672cb-3577-4143-f0c0-e555aa017844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8135999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "#accs=accuracy_score(s,ysr1)+accuracy_score(s,ysr1)+accuracy_score(s,ysr1)+accuracy_score(s,ysr1)+accuracy_score(s,ysr1)\n",
        "#print(accs/5)\n",
        "accs=np.zeros(len(d))\n",
        "for i in range(len(d)):\n",
        "  accs[i]=accuracy_score(s,ysr[:,i])\n",
        "\n",
        "print(np.average(accs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkV-MZPfkk9d",
        "outputId": "ba26a4a6-1377-48b7-8cfb-79abb5f930c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "t_drrpQsiQ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Private Settings"
      ],
      "metadata": {
        "id": "PEqosrESZFUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_B(target_model,data1,data2,t,d,k):\n",
        "  n=len(data1)\n",
        "  if k>=kv:\n",
        "    translate_images=translate_images2\n",
        "  else:\n",
        "    translate_images=translate_images1\n",
        "  datat1=translate_images(data1,d)\n",
        "  datat2=translate_images(data2,d)\n",
        "  ytr,indx=calc_attack_matrix(datat1,datat2)\n",
        "  ytr=np.argmax(ytr,axis=1)\n",
        "  ytr=ytr.reshape(n,1)\n",
        "  btr=(ytr-t)\n",
        "  btr=np.where(btr==0,1,0)\n",
        "  ytr=ytr.reshape(len(ytr))\n",
        "  btr=btr.reshape(len(btr))\n",
        "  indx=indx.reshape(len(indx))\n",
        "  return btr,ytr,indx"
      ],
      "metadata": {
        "id": "LGuJNQ8UZG8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=yt.reshape(dn,1)\n",
        "s=ys.reshape(dn,1)\n",
        "T=train1.reshape(dn,32*32*3)\n",
        "S=test1.reshape(dn,32*32*3)"
      ],
      "metadata": {
        "id": "zQ2hFlPqXuQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "btr=np.zeros((len(t),len(d)))\n",
        "ytr=np.zeros((len(t),len(d)))\n",
        "indxt=np.zeros((len(t),len(d)))\n",
        "print('Calculate features for training data')\n",
        "for i in range(len(d)):\n",
        "  btr[:,i],ytr[:,i],indxt[:,i]=calc_B(target_model,train1,trainx1,t,d[i],i)\n",
        "\n",
        "print('Calculate features for testing data')\n",
        "bsr=np.zeros((len(t),len(d)))\n",
        "ysr=np.zeros((len(t),len(d)))\n",
        "indxs=np.zeros((len(t),len(d)))\n",
        "for i in range(len(d)):\n",
        "  bsr[:,i],ysr[:,i],indxs[:,i]=calc_B(target_model,test1,testx1,s,d[i],i)\n",
        "\n",
        "\n",
        "print('rearrange b')\n",
        "b1=np.vstack((btr,bsr))\n",
        "b1.shape"
      ],
      "metadata": {
        "id": "U-rHKHDgYam1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total=np.concatenate([t, s])\n",
        "Btotal=np.hstack((b1,total))\n",
        "Btotal.shape"
      ],
      "metadata": {
        "id": "rSvZaq48Zuy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b09427-9b03-4a1f-868b-2bb95846b2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1=np.ones(dn)\n",
        "out2=np.zeros(dn)\n",
        "label1=np.hstack((out1,out2))\n",
        "print(label1.shape)\n",
        "label=label1.flatten()\n",
        "#label=to_categorical(label1)"
      ],
      "metadata": {
        "id": "tEooeIpnVudP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54b5973-a02b-4fb8-ba0c-be302cfc008d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "mem=modelLB.predict(Btotal)\n",
        "mem=np.argmax(mem,axis=1)\n",
        "#s_label_target=np.argmax(s_label,axis=1)\n",
        "accuracy_score(mem,label)"
      ],
      "metadata": {
        "id": "40WxnGKKZ2Kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fdd9bf0-07f5-4ebb-dbd9-155a2b78c13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5114"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Auc Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(mem, label, pos_label=1)\n",
        "metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "clTHOLmsZ4iI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68270da7-38b4-4b99-9e33-7b22a362b8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5134056807348233"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "#acct=accuracy_score(t,ytr1)+accuracy_score(t,ytr2)+accuracy_score(t,ytr3)+accuracy_score(t,ytr4)+accuracy_score(t,ytr5)\n",
        "#print(acct/5)\n",
        "acct=np.zeros(len(d))\n",
        "for i in range(len(d)):\n",
        "  acct[i]=accuracy_score(t,ytr[:,i])\n",
        "\n",
        "print(np.average(acct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkKQwUq3lpoi",
        "outputId": "f3568cf3-a4eb-4fa7-a735-472f531c036c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.69104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "#acct=accuracy_score(t,ytr1)+accuracy_score(t,ytr2)+accuracy_score(t,ytr3)+accuracy_score(t,ytr4)+accuracy_score(t,ytr5)\n",
        "#print(acct/5)\n",
        "accs=np.zeros(len(d))\n",
        "for i in range(len(d)):\n",
        "  accs[i]=accuracy_score(s,ysr[:,i])\n",
        "\n",
        "print(np.average(accs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtcWaBMLlzi8",
        "outputId": "3d3706b6-a55b-4fd8-825c-bbb4319fed29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(abs(fpr-tpr))"
      ],
      "metadata": {
        "id": "_THWiMwaZ7S-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08591cf6-14e4-4fe6-efa8-f66b05a97f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.026811361469646466"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate test accuracy/utility\n",
        "from sklearn.metrics import accuracy_score\n",
        "#test_labeld_result=np.argmax(prob_test,axis=1)\n",
        "#acct=accuracy_score(t,ytr1)+accuracy_score(t,ytr2)+accuracy_score(t,ytr3)+accuracy_score(t,ytr4)+accuracy_score(t,ytr5)\n",
        "#print(acct/5)\n",
        "accindx=np.zeros(len(d))\n",
        "for i in range(len(d)):\n",
        "  result_index=np.hstack((indxt[:,i],indxs[:,i]))\n",
        "  accindx[i]=accuracy_score(index_target,result_index)\n",
        "\n",
        "print(np.average(accindx))"
      ],
      "metadata": {
        "id": "_rPdBrxnLfrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop!!"
      ],
      "metadata": {
        "id": "ZODoEXTsJ6sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boundary Distance Attack"
      ],
      "metadata": {
        "id": "snJBoPNrZ-1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n1=5000"
      ],
      "metadata": {
        "id": "LrprnmaUaB-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_curve"
      ],
      "metadata": {
        "id": "lj98_F8xaFig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def continuous_rand_robust(model, dsx,dsy, max_samples=100, noise_samples=250, stddev=0.025, input_dim=shape,\n",
        "                           num=[1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 150, 250]): #350, 500, 750, 1000, 1500, 2000, 2500]):\n",
        "  \"\"\"Calculate robustness to random noise for Adv-x MI attack on continuous-featureed datasets (+ UCI adult).\n",
        "  :param model: model to approximate distances on (attack).\n",
        "  :param ds: tf dataset should be either the training set or the test set.\n",
        "  :param max_samples: maximum number of samples to take from the ds\n",
        "  :param noise_samples: number of noised samples to take for each sample in the ds.\n",
        "  :param stddev: the standard deviation to use for Gaussian noise (only for Adult, which has some continuous features)\n",
        "  :param input_dim: dimension of inputs for the dataset.\n",
        "  :param num: subnumber of samples to evaluate. max number is noise_samples\n",
        "  :return: a list of lists. each sublist of the accuracy on up to $num noise_samples.\n",
        "  \"\"\"\n",
        "  robust_accs = [[] for _ in num]\n",
        "  iter=len(dsx)\n",
        "  labels=dsy\n",
        "  for i in range(iter):\n",
        "    x=dsx[i:i+1]\n",
        "    noise = stddev * np.random.randn(noise_samples, 32,32,3)\n",
        "    x_noisy = np.clip(x + noise, 0, 1)\n",
        "    #print(noise.shape)\n",
        "    prob=model.predict(x_noisy)\n",
        "    pred=np.argmax(prob,axis=1)\n",
        "    #print(pred.shape)\n",
        "    for idx, n in enumerate(num):\n",
        "            if n == 0:\n",
        "              robust_accs[idx].append(1)\n",
        "            else:\n",
        "              robust_accs[idx].append(np.mean(pred[:n] == labels[i]))\n",
        "  return robust_accs"
      ],
      "metadata": {
        "id": "N0-5cCxvaIFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def continuous_rand_robust_target(model, dsx,dsy,dX, max_samples=100, noise_samples=250, stddev=0.025, input_dim=shape,\n",
        "                           num=[1, 2, 3, 4, 5, 10, 20,30, 40, 50, 75, 100, 150, 250]):#350, 500, 750, 1000, 1500, 2000, 2500]):\n",
        "  \"\"\"Calculate robustness to random noise for Adv-x MI attack on continuous-featureed datasets (+ UCI adult).\n",
        "  :param model: model to approximate distances on (attack).\n",
        "  :param ds: tf dataset should be either the training set or the test set.\n",
        "  :param max_samples: maximum number of samples to take from the ds\n",
        "  :param noise_samples: number of noised samples to take for each sample in the ds.\n",
        "  :param stddev: the standard deviation to use for Gaussian noise (only for Adult, which has some continuous features)\n",
        "  :param input_dim: dimension of inputs for the dataset.\n",
        "  :param num: subnumber of samples to evaluate. max number is noise_samples\n",
        "  :return: a list of lists. each sublist of the accuracy on up to $num noise_samples.\n",
        "  \"\"\"\n",
        "  robust_accs = [[] for _ in num]\n",
        "  iter=len(dsx)\n",
        "  labels=dsy\n",
        "  for i in range(iter):\n",
        "    x=dsx[i:i+1]\n",
        "    X=dX[i:i+1]\n",
        "    noise = stddev * np.random.randn(noise_samples, 32,32,3)\n",
        "    x_noisy = np.clip(x + noise, 0, 1)\n",
        "    X_noisy=np.clip(X + noise, 0, 255)\n",
        "    #print(noise.shape)\n",
        "    print('error1')\n",
        "    prob,indx=calc_attack_matrix(x_noisy,X_noisy)\n",
        "    pred=np.argmax(prob,axis=1)\n",
        "    print('error2')\n",
        "    #print(pred)\n",
        "    for idx, n in enumerate(num):\n",
        "            if n == 0:\n",
        "              robust_accs[idx].append(1)\n",
        "            else:\n",
        "              robust_accs[idx].append(np.mean(pred[:n] == labels[i]))\n",
        "  return robust_accs"
      ],
      "metadata": {
        "id": "v4HNiExaaKnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "non-Private"
      ],
      "metadata": {
        "id": "bjZfu9l1J-vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_source_in=continuous_rand_robust(target_model,train_data[0:n1],train_labels[0:n1])\n",
        "noise_source_out=continuous_rand_robust(target_model,test_data[0:n1],test_labels[0:n1])"
      ],
      "metadata": {
        "id": "HOxmfxDWasYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "private"
      ],
      "metadata": {
        "id": "IGTMp9ZPKCBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "noise_target_in=continuous_rand_robust_target(model_np,train_data[n1:n1*2],train_labels[n1:n1*2],trainX1[n1:n1*2])\n",
        "noise_target_out=continuous_rand_robust_target(model_np,test_data[n1:n1*2],test_labels[n1:n1*2],testX1[n1:n1*2])"
      ],
      "metadata": {
        "id": "DREJWDITbCDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_m = np.concatenate([np.ones(n1),np.zeros(n1)], axis=0)\n",
        "target_m = np.concatenate([np.ones(n1),np.zeros(n1)], axis=0)"
      ],
      "metadata": {
        "id": "A4B6mR2WbPbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_accuracy(y_true, probs, thresholds=None):\n",
        "  \"\"\"Return the max accuracy possible given the correct labels and guesses. Will try all thresholds unless passed.\n",
        "  Args:\n",
        "    y_true: True label of `in' or `out' (member or non-member, 1/0)\n",
        "    probs: The scalar to threshold\n",
        "    thresholds: In a blackbox setup with a shadow/source model, the threshold obtained by the source model can be passed\n",
        "      here for attackin the target model. This threshold will then be used.\n",
        "  Returns: max accuracy possible, accuracy at the threshold passed (if one was passed), the max precision possible,\n",
        "   and the precision at the threshold passed.\n",
        "  \"\"\"\n",
        "  attack_adv=[]\n",
        "  if thresholds is None:\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
        "    attack_adv.append(abs(tpr-fpr))\n",
        "    \n",
        "\n",
        "  accuracy_scores = []\n",
        "  precision_scores = []\n",
        "  for thresh in thresholds:\n",
        "    accuracy_scores.append(accuracy_score(y_true,\n",
        "                                          [1 if m > thresh else 0 for m in probs]))\n",
        "    precision_scores.append(precision_score(y_true, [1 if m > thresh else 0 for m in probs]))\n",
        "\n",
        "  accuracies = np.array(accuracy_scores)\n",
        "  precisions = np.array(precision_scores)\n",
        "  attack_adv=np.array(attack_adv)\n",
        "  max_accuracy = accuracies.max()\n",
        "  max_precision = precisions.max()\n",
        "  #print(type(attack_adv))\n",
        "  #print(attack_adv)\n",
        "  if (attack_adv.size !=0):\n",
        "      max_adv=np.max(attack_adv)\n",
        "      print(max_adv)\n",
        "  max_accuracy_threshold = thresholds[accuracies.argmax()]\n",
        "  max_precision_threshold = thresholds[precisions.argmax()]\n",
        "  return max_accuracy, max_accuracy_threshold, max_precision, max_precision_threshold"
      ],
      "metadata": {
        "id": "HD4zAX3tbXdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_threshold(source_m, source_stats, target_m, target_stats):\n",
        "  \"\"\" Train a threshold attack model and get teh accuracy on source and target models.\n",
        "  Args:\n",
        "    source_m: membership labels for source dataset (1 for member, 0 for non-member)\n",
        "    source_stats: scalar values to threshold (attack features) for source dataset\n",
        "    target_m: membership labels for target dataset (1 for member, 0 for non-member)\n",
        "    target_stats: scalar values to threshold (attack features) for target dataset\n",
        "  Returns: best acc from source thresh, precision @ same threshold, threshold for best acc,\n",
        "    precision at the best threshold for precision. all tuned on source model.\n",
        "  \"\"\"\n",
        "  # find best threshold on source data\n",
        "  acc_source, t, prec_source, tprec = get_max_accuracy(source_m, source_stats)\n",
        "\n",
        "  # find best accuracy on test data (just to check how much we overfit)\n",
        "  acc_test, _, prec_test, _ = get_max_accuracy(target_m, target_stats)\n",
        "\n",
        "  # get the test accuracy at the threshold selected on the source data\n",
        "  acc_test_t, _, _, _ = get_max_accuracy(target_m, target_stats, thresholds=[t])\n",
        "  _, _, prec_test_t, _ = get_max_accuracy(target_m, target_stats, thresholds=[tprec])\n",
        "  print(\"acc src: {}, acc test (best thresh): {}, acc test (src thresh): {}, thresh: {}\".format(acc_source, acc_test,\n",
        "                                                                                                acc_test_t, t))\n",
        "  print(\n",
        "    \"prec src: {}, prec test (best thresh): {}, prec test (src thresh): {}, thresh: {}\".format(prec_source, prec_test,\n",
        "                                                                                               prec_test_t, tprec))\n",
        "\n",
        "  return acc_test_t, prec_test_t, t, tprec"
      ],
      "metadata": {
        "id": "rE2VC65Uba4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(noise_source_in)):\n",
        "            noise_source = np.concatenate([noise_source_in[i], noise_source_out[i]], axis=0)\n",
        "            noise_target = np.concatenate([noise_target_in[i], noise_target_out[i]], axis=0)\n",
        "            get_threshold(source_m, noise_source, target_m, noise_target)"
      ],
      "metadata": {
        "id": "5cfNEFwCbehR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Evaluation_of_MIAShield_(Cifar10_and_100).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}